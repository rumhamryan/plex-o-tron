============================= test session starts ==============================
platform linux -- Python 3.12.12, pytest-9.0.2, pluggy-1.6.0 -- /home/jules/.pyenv/versions/3.12.12/bin/python
cachedir: .pytest_cache
rootdir: /app
configfile: pyproject.toml
testpaths: tests
plugins: mock-3.15.1, anyio-4.12.0, asyncio-1.3.0
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collecting ... collected 157 items

tests/handlers/test_callback_handlers.py::test_button_handler_routes_search PASSED [  0%]
tests/handlers/test_callback_handlers.py::test_button_handler_routes_delete PASSED [  1%]
tests/handlers/test_callback_handlers.py::test_button_handler_routes_download PASSED [  1%]
tests/handlers/test_command_handlers.py::test_search_command_starts_workflow PASSED [  2%]
tests/handlers/test_command_handlers.py::test_plex_status_command_calls_service PASSED [  3%]
tests/handlers/test_error_handler.py::test_global_error_handler_logs_and_notifies PASSED [  3%]
tests/handlers/test_message_handlers.py::test_handle_link_message_processes_input PASSED [  4%]
tests/handlers/test_message_handlers.py::test_handle_search_message_routes_search PASSED [  5%]
tests/handlers/test_message_handlers.py::test_handle_search_message_routes_delete PASSED [  5%]
tests/services/test_download_manager.py::test_progress_reporter_movie PASSED [  6%]
tests/services/test_download_manager.py::test_progress_reporter_tv_paused PASSED [  7%]
tests/services/test_download_manager.py::test_progress_reporter_season_pack_omits_episode_names PASSED [  7%]
tests/services/test_download_manager.py::test_progress_reporter_skips_when_cancellation_pending PASSED [  8%]
tests/services/test_download_manager.py::test_download_task_wrapper_success PASSED [  8%]
tests/services/test_download_manager.py::test_download_task_wrapper_cancellation_cleanup PASSED [  9%]
tests/services/test_download_manager.py::test_download_task_wrapper_failure_message PASSED [ 10%]
tests/services/test_download_manager.py::test_download_with_progress_http_status_error PASSED [ 10%]
tests/services/test_download_manager.py::test_download_with_progress_reports_during_pause PASSED [ 11%]
tests/services/test_download_manager.py::test_add_download_to_queue PASSED [ 12%]
tests/services/test_download_manager.py::test_add_season_to_queue PASSED [ 12%]
tests/services/test_download_manager.py::test_process_queue_for_user_active PASSED [ 13%]
tests/services/test_download_manager.py::test_process_queue_for_user_start PASSED [ 14%]
tests/services/test_download_manager.py::test_handle_pause_resume_toggles_state PASSED [ 14%]
tests/services/test_download_manager.py::test_cancel_request_flag_flow PASSED [ 15%]
tests/services/test_dry_run_integration.py::test_dry_run_flow_uses_wiki_year_and_filters_resolution FAILED [ 15%]
tests/services/test_dry_run_integration.py::test_dry_run_movie_explicit_year_overrides_wiki FAILED [ 16%]
tests/services/test_dry_run_integration.py::test_dry_run_tv_search_workflow_basic FAILED [ 17%]
tests/services/test_generic_torrent_scraper_config_cache.py::test_load_site_config_uses_cache PASSED [ 17%]
tests/services/test_generic_torrent_scraper_filtering.py::test_two_stage_filtering_prefers_precise_single_token_match FAILED [ 18%]
tests/services/test_generic_torrent_scraper_logging.py::test_fetch_page_logs_status PASSED [ 19%]
tests/services/test_generic_torrent_scraper_logging.py::test_fetch_page_logs_error_body PASSED [ 19%]
tests/services/test_generic_torrent_scraper_magnet.py::test_search_parses_magnet_link_from_detail_page FAILED [ 20%]
tests/services/test_generic_torrent_scraper_topn.py::test_extract_data_from_row FAILED [ 21%]
tests/services/test_generic_torrent_scraper_topn.py::test_parse_and_select_top_results FAILED [ 21%]
tests/services/test_media_manager.py::test_generate_plex_filename_movie PASSED [ 22%]
tests/services/test_media_manager.py::test_generate_plex_filename_tv_with_episode_title PASSED [ 22%]
tests/services/test_media_manager.py::test_generate_plex_filename_illegal_chars PASSED [ 23%]
tests/services/test_media_manager.py::test_parse_resolution_from_name[Movie.2160p.BluRay-4K] PASSED [ 24%]
tests/services/test_media_manager.py::test_parse_resolution_from_name[Video.1080p.x265-1080p] PASSED [ 24%]
tests/services/test_media_manager.py::test_parse_resolution_from_name[Series.720p.HDTV-720p] PASSED [ 25%]
tests/services/test_media_manager.py::test_parse_resolution_from_name[Old.Movie.DVDRip-SD] PASSED [ 26%]
tests/services/test_media_manager.py::test_parse_resolution_from_name[Sample-N/A] PASSED [ 26%]
tests/services/test_media_manager.py::test_handle_successful_download PASSED [ 27%]
tests/services/test_media_manager.py::test_handle_successful_download_season_pack PASSED [ 28%]
tests/services/test_plex_service.py::test_get_plex_server_status_connected PASSED [ 28%]
tests/services/test_plex_service.py::test_get_plex_server_status_unauthorized PASSED [ 29%]
tests/services/test_plex_service.py::test_get_plex_server_status_connection_error PASSED [ 29%]
tests/services/test_scraping_service.py::test_fetch_episode_title_dedicated_page FAILED [ 30%]
tests/services/test_scraping_service.py::test_fetch_episode_title_strips_miniseries_suffix FAILED [ 31%]
tests/services/test_scraping_service.py::test_fetch_episode_title_strips_tv_series_suffix FAILED [ 31%]
tests/services/test_scraping_service.py::test_fetch_episode_title_embedded_page FAILED [ 32%]
tests/services/test_scraping_service.py::test_fetch_episode_title_not_found FAILED [ 33%]
tests/services/test_scraping_service.py::test_fetch_season_episode_count FAILED [ 33%]
tests/services/test_scraping_service.py::test_fetch_season_episode_count_prefers_titles_over_overview FAILED [ 34%]
tests/services/test_scraping_service.py::test_fetch_season_episode_count_skips_ongoing_overview FAILED [ 35%]
tests/services/test_scraping_service.py::test_scrape_1337x_parses_results FAILED [ 35%]
tests/services/test_scraping_service.py::test_scrape_1337x_no_results FAILED [ 36%]
tests/services/test_scraping_service.py::test_scrape_1337x_fuzzy_filter FAILED [ 36%]
tests/services/test_scraping_service.py::test_scrape_1337x_passes_limit FAILED [ 37%]
tests/services/test_scraping_service.py::test_scrape_yts_parses_results FAILED [ 38%]
tests/services/test_scraping_service.py::test_scrape_yts_retries_on_validation_failure FAILED [ 38%]
tests/services/test_scraping_service.py::test_scrape_yts_paginates_browse_pages_to_find_year FAILED [ 39%]
tests/services/test_scraping_service.py::test_scrape_yts_api_fallback_relaxes_quality FAILED [ 40%]
tests/services/test_scraping_service.py::test_scrape_yts_api_fallback_relaxes_year FAILED [ 40%]
tests/services/test_scraping_service.py::test_scrape_yts_token_gate_avoids_near_homonyms FAILED [ 41%]
tests/services/test_scraping_service.py::test_strategy_find_direct_links_magnet FAILED [ 42%]
tests/services/test_scraping_service.py::test_strategy_find_direct_links_torrent FAILED [ 42%]
tests/services/test_scraping_service.py::test_strategy_find_direct_links_none FAILED [ 43%]
tests/services/test_scraping_service.py::test_strategy_contextual_search_keyword FAILED [ 43%]
tests/services/test_scraping_service.py::test_strategy_contextual_search_query_match FAILED [ 44%]
tests/services/test_scraping_service.py::test_strategy_contextual_search_unrelated_keyword FAILED [ 45%]
tests/services/test_scraping_service.py::test_strategy_find_in_tables_single_match FAILED [ 45%]
tests/services/test_scraping_service.py::test_strategy_find_in_tables_multiple_matches FAILED [ 46%]
tests/services/test_scraping_service.py::test_strategy_find_in_tables_ignores_unrelated_tables FAILED [ 47%]
tests/services/test_scraping_service.py::test_score_candidate_links_prefers_magnet FAILED [ 47%]
tests/services/test_scraping_service.py::test_score_candidate_links_penalizes_ads FAILED [ 48%]
tests/services/test_scraping_service.py::test_score_candidate_links_prefers_better_match FAILED [ 49%]
tests/services/test_search_logic.py::test_parse_codec[Some.Movie.x265-x265] PASSED [ 49%]
tests/services/test_search_logic.py::test_parse_codec[Another HEVC release-x265] PASSED [ 50%]
tests/services/test_search_logic.py::test_parse_codec[Film X264 edition-x264] PASSED [ 50%]
tests/services/test_search_logic.py::test_parse_codec[AV1 Showcase 1080p-av1] PASSED [ 51%]
tests/services/test_search_logic.py::test_parse_codec[H.265 encode-x265] PASSED [ 52%]
tests/services/test_search_logic.py::test_parse_codec[H 265 Hybrid-x265] PASSED [ 52%]
tests/services/test_search_logic.py::test_parse_codec[H264 remux-x264] PASSED [ 53%]
tests/services/test_search_logic.py::test_parse_codec[H 264-FLUX-x264] PASSED [ 54%]
tests/services/test_search_logic.py::test_parse_codec[Rick and Morty S08E01 1080p WEB H264-LAZYCUNTS [eztv]-x264] PASSED [ 54%]
tests/services/test_search_logic.py::test_parse_codec[Rick and Morty S08E01 Summer of All Fears 720p MAX WEB-DL DDP5 1 H 264-FLUX [eztv]-x264] PASSED [ 55%]
tests/services/test_search_logic.py::test_parse_codec[No codec here-None] PASSED [ 56%]
tests/services/test_search_logic.py::test_parse_size_to_bytes[1.5 GB-1610612736.0] PASSED [ 56%]
tests/services/test_search_logic.py::test_parse_size_to_bytes[500 MB-524288000] PASSED [ 57%]
tests/services/test_search_logic.py::test_parse_size_to_bytes[1024 KB-1048576] PASSED [ 57%]
tests/services/test_search_logic.py::test_parse_size_to_bytes[invalid-0] PASSED [ 58%]
tests/services/test_search_logic.py::test_score_torrent_result PASSED    [ 59%]
tests/services/test_search_orchestration.py::test_orchestrate_searches_calls_sites_and_sorts FAILED [ 59%]
tests/services/test_search_orchestration.py::test_orchestrate_searches_respects_enabled_flag FAILED [ 60%]
tests/services/test_search_orchestration.py::test_orchestrate_searches_yaml_fallback_for_unknown_site FAILED [ 61%]
tests/services/test_torrent_service.py::test_process_user_input_magnet_routing PASSED [ 61%]
tests/services/test_torrent_service.py::test_process_user_input_torrent_url_routing PASSED [ 62%]
tests/services/test_torrent_service.py::test_process_user_input_webpage_routing PASSED [ 63%]
tests/services/test_torrent_service.py::test_handle_webpage_url_no_links PASSED [ 63%]
tests/services/test_torrent_service.py::test_handle_webpage_url_multiple_links PASSED [ 64%]
tests/services/test_torrent_service.py::test_fetch_metadata_from_magnet_timeout PASSED [ 64%]
tests/services/test_torrent_service.py::test_fetch_metadata_from_magnet_success PASSED [ 65%]
tests/test_batch_scanning.py::test_update_batch_triggers_single_scan PASSED [ 66%]
tests/test_batch_scanning.py::test_update_batch_no_scan_before_completion PASSED [ 66%]
tests/test_batch_scanning.py::test_update_batch_skip_duplicate_scan PASSED [ 67%]
tests/test_config.py::test_get_configuration_happy_path PASSED           [ 68%]
tests/test_config.py::test_get_configuration_missing_file PASSED         [ 68%]
tests/test_config.py::test_get_configuration_missing_token PASSED        [ 69%]
tests/test_config.py::test_get_configuration_missing_default_path PASSED [ 70%]
tests/test_config.py::test_get_configuration_invalid_search_json PASSED  [ 70%]
tests/test_state.py::test_save_and_load_state_roundtrip PASSED           [ 71%]
tests/test_state.py::test_load_state_missing_file PASSED                 [ 71%]
tests/test_state.py::test_load_state_json_error PASSED                   [ 72%]
tests/test_state.py::test_post_init_resumes_persisted_download PASSED    [ 73%]
tests/test_state.py::test_post_shutdown_cancels_tasks_and_saves_state PASSED [ 73%]
tests/test_utils.py::test_format_bytes[0-0B] PASSED                      [ 74%]
tests/test_utils.py::test_format_bytes[1023-1023.0 B] PASSED             [ 75%]
tests/test_utils.py::test_format_bytes[1024-1.0 KB] PASSED               [ 75%]
tests/test_utils.py::test_format_bytes[1536-1.5 KB] PASSED               [ 76%]
tests/test_utils.py::test_format_bytes[1048576-1.0 MB] PASSED            [ 77%]
tests/test_utils.py::test_format_bytes[1610612736-1.5 GB] PASSED         [ 77%]
tests/test_utils.py::test_extract_first_int[S01E05-1] PASSED             [ 78%]
tests/test_utils.py::test_extract_first_int[Season 12-12] PASSED         [ 78%]
tests/test_utils.py::test_extract_first_int[No numbers here-None] PASSED [ 79%]
tests/test_utils.py::test_extract_first_int[-None] PASSED                [ 80%]
tests/test_utils.py::test_extract_first_int[Episode 5 is the best-5] PASSED [ 80%]
tests/test_utils.py::test_parse_torrent_name[Movie.Title.2023-expected0] PASSED [ 81%]
tests/test_utils.py::test_parse_torrent_name[Show.Name.S01E02.1080p-expected1] PASSED [ 82%]
tests/test_utils.py::test_parse_torrent_name[Show Name 1x02 [1080p]-expected2] PASSED [ 82%]
tests/test_utils.py::test_parse_torrent_name[Another_Show-S01E02_[x265]-expected3] PASSED [ 83%]
tests/test_utils.py::test_parse_torrent_name[Unknown.File[x265]-expected4] PASSED [ 84%]
tests/utils/test_safe_edit_message_fallback.py::test_safe_edit_message_falls_back_to_send PASSED [ 84%]
tests/utils/test_safe_edit_message_retry_after.py::test_safe_edit_message_retries_on_small_retry_after PASSED [ 85%]
tests/utils/test_safe_edit_message_retry_after.py::test_safe_edit_message_suppresses_on_large_retry_after PASSED [ 85%]
tests/utils/test_safe_send_message.py::test_safe_send_message_success PASSED [ 86%]
tests/utils/test_safe_send_message.py::test_safe_send_message_retries_on_timeout_then_succeeds PASSED [ 87%]
tests/utils/test_safe_send_message.py::test_safe_send_message_respects_retry_after PASSED [ 87%]
tests/utils/test_safe_send_message.py::test_safe_send_message_retries_on_network_error_then_raises PASSED [ 88%]
tests/workflows/test_delete_workflow.py::test_delete_show_happy_path PASSED [ 89%]
tests/workflows/test_delete_workflow.py::test_delete_workflow_not_found PASSED [ 89%]
tests/workflows/test_search_workflow.py::test_search_movie_happy_path PASSED [ 90%]
tests/workflows/test_search_workflow.py::test_search_tv_happy_path PASSED [ 91%]
tests/workflows/test_search_workflow.py::test_search_cancel_clears_context PASSED [ 91%]
tests/workflows/test_search_workflow.py::test_resolution_filters_results[search_resolution_4k-expected_titles0] PASSED [ 92%]
tests/workflows/test_search_workflow.py::test_resolution_filters_results[search_resolution_1080p-expected_titles1] PASSED [ 92%]
tests/workflows/test_search_workflow.py::test_tv_season_reply_offers_scope_buttons PASSED [ 93%]
tests/workflows/test_search_workflow.py::test_handle_tv_scope_selection_single PASSED [ 94%]
tests/workflows/test_search_workflow.py::test_handle_tv_scope_selection_season FAILED [ 94%]
tests/workflows/test_search_workflow.py::test_handle_tv_scope_selection_season_fallback FAILED [ 95%]
tests/workflows/test_search_workflow.py::test_present_season_download_confirmation PASSED [ 96%]
tests/workflows/test_search_workflow.py::test_present_season_download_confirmation_pack PASSED [ 96%]
tests/workflows/test_search_workflow.py::test_present_season_download_confirmation_pack_has_reject_button PASSED [ 97%]
tests/workflows/test_search_workflow.py::test_handle_reject_season_pack_triggers_individual PASSED [ 98%]
tests/workflows/test_search_workflow.py::test_entire_season_skips_pack_and_targets_missing FAILED [ 98%]
tests/workflows/test_search_workflow.py::test_entire_season_all_owned_exits_early FAILED [ 99%]
tests/workflows/test_search_workflow_integration.py::test_tv_season_fallback_uses_wiki_titles_and_corrected_title FAILED [100%]

=================================== FAILURES ===================================
___________ test_dry_run_flow_uses_wiki_year_and_filters_resolution ____________

mocker = <pytest_mock.plugin.MockerFixture object at 0x7ff9863811c0>

    @pytest.mark.asyncio
    async def test_dry_run_flow_uses_wiki_year_and_filters_resolution(mocker):
        # Mock Wikipedia years for 'Alien'
        mocker.patch(
            "telegram_bot.services.scrapers.wikipedia_scraper.fetch_movie_years_from_wikipedia",
            new=AsyncMock(return_value=([1979], None)),
        )

        # Mock scrapers
        m_yts = mocker.patch.object(
            YtsScraper,
            "search",
>           new=AsyncMock(return_value=yts_results),
                                       ^^^^^^^^^^^
        )
E       NameError: name 'yts_results' is not defined

tests/services/test_dry_run_integration.py:80: NameError
_______________ test_dry_run_movie_explicit_year_overrides_wiki ________________

mocker = <pytest_mock.plugin.MockerFixture object at 0x7ff98640da90>

    @pytest.mark.asyncio
    async def test_dry_run_movie_explicit_year_overrides_wiki(mocker):
        # Wikipedia would return an incorrect or different year, but explicit year should win
        mocker.patch(
            "telegram_bot.services.scrapers.wikipedia_scraper.fetch_movie_years_from_wikipedia",
            new=AsyncMock(return_value=([1983], None)),
        )

        yts_results = [
            {
                "title": "The Thing (1982) 1080p WEB x265 [YTS]",
                "score": 22,
                "source": "YTS.mx",
            },
        ]
        x_results = [
            {"title": "The.Thing.1982.1080p.BluRay.x265", "score": 18, "source": "1337x"},
        ]
        m_yts = mocker.patch.object(
            YtsScraper,
            "search",
            new=AsyncMock(return_value=yts_results),
        )
        m_1337 = mocker.patch(
            "telegram_bot.services.scrapers.torrent_scraper.scrape_1337x",
            new=AsyncMock(return_value=x_results),
        )

        ctx = _ctx_with_search_config()
        title = "The Thing 1982"
        resolution = "1080p"

        # Emulate dry-run logic: extract explicit year and base
        import re as _re

        m = _re.search(r"\b(19\d{2}|20\d{2})\b", title)
        explicit_year = m.group(1) if m else None
        base = title[: m.start()].strip() if m else title

        years, corrected = await fetch_movie_years_from_wikipedia(base)
        base_for_search = corrected or base
>       assert years == [1983]
E       AssertionError: assert [1982, 2011] == [1983]
E
E         At index 0 diff: 1982 != 1983
E         Left contains one more item: 2011
E
E         Full diff:
E           [
E         -     1983,...
E
E         ...Full output truncated (5 lines hidden), use '-vv' to show

tests/services/test_dry_run_integration.py:152: AssertionError
____________________ test_dry_run_tv_search_workflow_basic _____________________

mocker = <pytest_mock.plugin.MockerFixture object at 0x7ff98640e150>

    @pytest.mark.asyncio
    async def test_dry_run_tv_search_workflow_basic(mocker):
        # TV workflow uses TV websites set; here we only include 1337x TV category
        ctx = Mock()
        ctx.bot_data = {
            "SEARCH_CONFIG": {
                "websites": {
                    "movies": [],
                    "tv": [
                        {
                            "name": "1337x",
                            "enabled": True,
                            "search_url": "https://1337x.to/category-search/{query}/TV/1/",
                        }
                    ],
                },
                "preferences": {
                    "tv": {
                        "resolutions": {"1080p": 5, "720p": 1},
                        "codecs": {"x265": 4, "hevc": 4, "x264": 1, "h264": 1},
                        "uploaders": {"EZTV": 5, "MeGusta": 5},
                    }
                },
            }
        }

        x_results = [
            {"title": "My.Show.S01E01.1080p.WEB.x265", "score": 17, "source": "1337x"},
            {"title": "My.Show.S01E01.720p.WEB.x264", "score": 9, "source": "1337x"},
        ]

        m_1337 = mocker.patch(
            "telegram_bot.services.scrapers.torrent_scraper.scrape_1337x",
            new=AsyncMock(return_value=x_results),
        )

        query = "My Show S01E01"
        resolution = "1080p"
>       results = await orchestrate_searches(query, "tv", ctx, resolution=resolution)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/services/test_dry_run_integration.py:205:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

query = 'My Show S01E01', media_type = 'tv'
context = <Mock id='140709675933840'>, kwargs = {'resolution': '1080p'}
search_config = {'preferences': {'tv': {'codecs': {'h264': 1, 'hevc': 4, 'x264': 1, 'x265': 4}, 'resolutions': {'1080p': 5, '720p': 1}...vies': [], 'tv': [{'enabled': True, 'name': '1337x', 'search_url': 'https://1337x.to/category-search/{query}/TV/1/'}]}}
websites_config = {'movies': [], 'tv': [{'enabled': True, 'name': '1337x', 'search_url': 'https://1337x.to/category-search/{query}/TV/1/'}]}
config_key = 'tv'
sites_to_scrape = [{'enabled': True, 'name': '1337x', 'search_url': 'https://1337x.to/category-search/{query}/TV/1/'}]
scraper_map = {'1337x': <function scrape_1337x at 0x7ff98673a0c0>}
yts_scraper = <telegram_bot.services.scrapers.torrent_scraper.YtsScraper object at 0x7ff986400860>
tasks = []

    async def orchestrate_searches(
        query: str, media_type: str, context: ContextTypes.DEFAULT_TYPE, **kwargs
    ) -> list[dict[str, Any]]:
        """
        Coordinates searches across all enabled torrent sites concurrently.

        It reads the search configuration, creates an asyncio task for each
        enabled scraper, runs them in parallel, and returns a single list of
        results sorted by score.
        """
        search_config = context.bot_data.get("SEARCH_CONFIG", {})
        websites_config = search_config.get("websites", {})

        config_key = "movies" if media_type == "movie" else "tv"
        sites_to_scrape = websites_config.get(config_key, [])

        if not isinstance(sites_to_scrape, list) or not sites_to_scrape:
            logger.warning(
                f"[SEARCH] No websites configured for media type '{config_key}' in config.ini."
            )
            return []

        # A dedicated scraper for EZTV would need to be created in the future.
        scraper_map: dict[str, ScraperFunction] = {
            "1337x": scrape_1337x,
        }

        yts_scraper = YtsScraper()


        tasks = []
        for site_info in sites_to_scrape:
            if not isinstance(site_info, dict):
                logger.warning(
                    f"[SEARCH] Skipping invalid item in '{config_key}' config: {site_info}"
                )
                continue

            if site_info.get("enabled", True):
                site_name = site_info.get("name")
                site_url = site_info.get("search_url")

                if not isinstance(site_name, str) or not site_name:
                    logger.warning(
                        f"[SEARCH] Skipping site due to missing or invalid 'name' key: {site_info}"
                    )
                    continue

                if not site_url:
                    logger.warning(
                        f"[SEARCH] Skipping site '{site_name}' due to missing 'search_url' key."
                    )
                    continue

                search_query = query
                year = kwargs.get("year")

                # Only append the year for the 1337x scraper.
                if site_name == "1337x" and year:
                    search_query += f" {year}"

                scraper_func = scraper_map.get(site_name)

                # Allow callers to override the string used for fuzzy filtering.
                base_filter = kwargs.get("base_query_for_filter", query)
                extra_kwargs = {
                    k: v for k, v in kwargs.items() if k != "base_query_for_filter"
                }

                if site_name == "YTS.mx":
                    logger.info(
                        f"[SEARCH] Creating search task for 'YTS.mx' with query: '{query}'"
                    )
                    task = asyncio.create_task(
                        yts_scraper.search(
                            search_query, media_type, context=context, **extra_kwargs
                        )
                    )
                    tasks.append(task)
                elif scraper_func is not None:
                    logger.info(
                        f"[SEARCH] Creating search task for '{site_name}' with query: '{query}'"
                    )
                    task = asyncio.create_task(
>                       scraper_func(
                            search_query,
                            media_type,
                            site_url,
                            context,
                            base_query_for_filter=base_filter,
                            **extra_kwargs,
                        )
                    )
E                   TypeError: scrape_1337x() takes 2 positional arguments but 4 were given

telegram_bot/services/search_logic.py:108: TypeError
_________ test_two_stage_filtering_prefers_precise_single_token_match __________

mocker = <pytest_mock.plugin.MockerFixture object at 0x7ff986403b90>

    @pytest.mark.asyncio
    async def test_two_stage_filtering_prefers_precise_single_token_match(mocker):
        site_config = {
            "site_name": "TestSite",
            "base_url": "https://example.com",
            "search_path": "/search/{query}/{category}/{page}/",
            "category_mapping": {"movie": "movies"},
            "results_page_selectors": {
                "result_row": "tr",
                "name": "td.name a",
                "magnet": "td.name a",
                "seeders": "td.seeds",
                "leechers": "td.leeches",
                "size": "td.size",
            },
            "matching": {"fuzz_scorer": "ratio", "fuzz_threshold": 40},
        }

        search_html = (
            "<table>"
            "<tr>"
            '<td class="name"><a href="magnet:?xt=1">Dune Part Two 2024 1080p</a></td>'
            '<td class="seeds">10</td><td class="leeches">1</td><td class="size">1 GB</td>'
            "</tr>"
            "<tr>"
            '<td class="name"><a href="magnet:?xt=2">Dune Part Two 2024 720p</a></td>'
            '<td class="seeds">8</td><td class="leeches">1</td><td class="size">900 MB</td>'
            "</tr>"
            "<tr>"
            '<td class="name"><a href="magnet:?xt=3">Dune 1984 1080p</a></td>'
            '<td class="seeds">5</td><td class="leeches">1</td><td class="size">1.2 GB</td>'
            "</tr>"
            "</table>"
        )

        fetch_mock = AsyncMock(return_value=search_html)
        mocker.patch.object(GenericTorrentScraper, "_fetch_page", fetch_mock)

        scraper = GenericTorrentScraper(site_config)
        results = await scraper.search("Dune", "movie")

        # With the updated precision logic, a single-token query like "Dune"
        # prefers exact token-equivalent titles over broader ones like
        # "Dune Part Two". Therefore, only the base title match remains.
>       assert len(results) == 1
E       assert 0 == 1
E        +  where 0 = len([])

tests/services/test_generic_torrent_scraper_filtering.py:51: AssertionError
------------------------------ Captured log call -------------------------------
ERROR    telegram_bot.config:generic_torrent_scraper.py:324 An unexpected error occurred in GenericTorrentScraper.search: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?
Traceback (most recent call last):
  File "/app/telegram_bot/services/generic_torrent_scraper.py", line 153, in search
    soup = BeautifulSoup(search_html, "lxml")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/bs4/__init__.py", line 366, in __init__
    raise FeatureNotFound(
bs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?
_______________ test_search_parses_magnet_link_from_detail_page ________________

mocker = <pytest_mock.plugin.MockerFixture object at 0x7ff9863f5760>

    @pytest.mark.asyncio
    async def test_search_parses_magnet_link_from_detail_page(mocker):
        """Ensure magnet links are extracted when only available on detail pages."""

        site_config = {
            "site_name": "TestSite",
            "base_url": "https://example.com",
            "search_path": "/search/{query}/{category}/{page}/",
            "category_mapping": {"movie": "movies"},
            "results_page_selectors": {
                "result_row": "tr",
                "name": "td.name a",
                "magnet": "td.name a",
                "seeders": "td.seeds",
                "leechers": "td.leeches",
                "size": "td.size",
            },
            "details_page_selectors": {"magnet_url": "a[href^='magnet:']"},
        }

        search_html = (
            "<table><tr>"
            '<td class="name"><a href="/torrent/1">Example</a></td>'
            '<td class="seeds">10</td>'
            '<td class="leeches">1</td>'
            '<td class="size">1 GB</td>'
            "</tr></table>"
        )
        detail_html = (
            "<html><body>"
            '<a href="magnet:?xt=urn:btih:abcdef&dn=Example">Magnet</a>'
            "</body></html>"
        )

        fetch_mock = AsyncMock(side_effect=[search_html, detail_html])
        mocker.patch.object(GenericTorrentScraper, "_fetch_page", fetch_mock)

        scraper = GenericTorrentScraper(site_config)
        results = await scraper.search("Example", "movie")

>       assert len(results) == 1
E       assert 0 == 1
E        +  where 0 = len([])

tests/services/test_generic_torrent_scraper_magnet.py:48: AssertionError
------------------------------ Captured log call -------------------------------
ERROR    telegram_bot.config:generic_torrent_scraper.py:324 An unexpected error occurred in GenericTorrentScraper.search: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?
Traceback (most recent call last):
  File "/app/telegram_bot/services/generic_torrent_scraper.py", line 153, in search
    soup = BeautifulSoup(search_html, "lxml")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/bs4/__init__.py", line 366, in __init__
    raise FeatureNotFound(
bs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?
__________________________ test_extract_data_from_row __________________________

    def test_extract_data_from_row() -> None:
        scraper = _build_scraper()
        row_html = (
            "<tr>"
            '<td class="name"><a href="magnet:?xt=1">Example</a></td>'
            '<td class="seeds">5</td>'
            '<td class="leeches">2</td>'
            '<td class="size">1 GB</td>'
            '<td class="uploader"><a>Uploader</a></td>'
            "</tr>"
        )
>       row = BeautifulSoup(row_html, "lxml").select_one("tr")
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/services/test_generic_torrent_scraper_topn.py:36:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <[AttributeError("'<class 'bs4.BeautifulSoup'>' object has no attribute 'contents'") raised in repr()] BeautifulSoup object at 0x7ff9863e7410>
markup = '<tr><td class="name"><a href="magnet:?xt=1">Example</a></td><td class="seeds">5</td><td class="leeches">2</td><td class="size">1 GB</td><td class="uploader"><a>Uploader</a></td></tr>'
features = ['lxml'], builder = None, parse_only = None, from_encoding = None
exclude_encodings = None, element_classes = None, kwargs = {}
deprecated_argument = <function BeautifulSoup.__init__.<locals>.deprecated_argument at 0x7ff9862b1e40>
original_builder = None, original_features = 'lxml', builder_class = None
possible_builder_class = None

    def __init__(
        self,
        markup: _IncomingMarkup = "",
        features: Optional[Union[str, Sequence[str]]] = None,
        builder: Optional[Union[TreeBuilder, Type[TreeBuilder]]] = None,
        parse_only: Optional[SoupStrainer] = None,
        from_encoding: Optional[_Encoding] = None,
        exclude_encodings: Optional[_Encodings] = None,
        element_classes: Optional[Dict[Type[PageElement], Type[PageElement]]] = None,
        **kwargs: Any,
    ):
        """Constructor.

        :param markup: A string or a file-like object representing
         markup to be parsed.

        :param features: Desirable features of the parser to be
         used. This may be the name of a specific parser ("lxml",
         "lxml-xml", "html.parser", or "html5lib") or it may be the
         type of markup to be used ("html", "html5", "xml"). It's
         recommended that you name a specific parser, so that
         Beautiful Soup gives you the same results across platforms
         and virtual environments.

        :param builder: A TreeBuilder subclass to instantiate (or
         instance to use) instead of looking one up based on
         `features`. You only need to use this if you've implemented a
         custom TreeBuilder.

        :param parse_only: A SoupStrainer. Only parts of the document
         matching the SoupStrainer will be considered. This is useful
         when parsing part of a document that would otherwise be too
         large to fit into memory.

        :param from_encoding: A string indicating the encoding of the
         document to be parsed. Pass this in if Beautiful Soup is
         guessing wrongly about the document's encoding.

        :param exclude_encodings: A list of strings indicating
         encodings known to be wrong. Pass this in if you don't know
         the document's encoding but you know Beautiful Soup's guess is
         wrong.

        :param element_classes: A dictionary mapping BeautifulSoup
         classes like Tag and NavigableString, to other classes you'd
         like to be instantiated instead as the parse tree is
         built. This is useful for subclassing Tag or NavigableString
         to modify default behavior.

        :param kwargs: For backwards compatibility purposes, the
         constructor accepts certain keyword arguments used in
         Beautiful Soup 3. None of these arguments do anything in
         Beautiful Soup 4; they will result in a warning and then be
         ignored.

         Apart from this, any keyword arguments passed into the
         BeautifulSoup constructor are propagated to the TreeBuilder
         constructor. This makes it possible to configure a
         TreeBuilder by passing in arguments, not just by saying which
         one to use.
        """
        if "convertEntities" in kwargs:
            del kwargs["convertEntities"]
            warnings.warn(
                "BS4 does not respect the convertEntities argument to the "
                "BeautifulSoup constructor. Entities are always converted "
                "to Unicode characters."
            )

        if "markupMassage" in kwargs:
            del kwargs["markupMassage"]
            warnings.warn(
                "BS4 does not respect the markupMassage argument to the "
                "BeautifulSoup constructor. The tree builder is responsible "
                "for any necessary markup massage."
            )

        if "smartQuotesTo" in kwargs:
            del kwargs["smartQuotesTo"]
            warnings.warn(
                "BS4 does not respect the smartQuotesTo argument to the "
                "BeautifulSoup constructor. Smart quotes are always converted "
                "to Unicode characters."
            )

        if "selfClosingTags" in kwargs:
            del kwargs["selfClosingTags"]
            warnings.warn(
                "Beautiful Soup 4 does not respect the selfClosingTags argument to the "
                "BeautifulSoup constructor. The tree builder is responsible "
                "for understanding self-closing tags."
            )

        if "isHTML" in kwargs:
            del kwargs["isHTML"]
            warnings.warn(
                "Beautiful Soup 4 does not respect the isHTML argument to the "
                "BeautifulSoup constructor. Suggest you use "
                "features='lxml' for HTML and features='lxml-xml' for "
                "XML."
            )

        def deprecated_argument(old_name: str, new_name: str) -> Optional[Any]:
            if old_name in kwargs:
                warnings.warn(
                    'The "%s" argument to the BeautifulSoup constructor '
                    'was renamed to "%s" in Beautiful Soup 4.0.0'
                    % (old_name, new_name),
                    DeprecationWarning,
                    stacklevel=3,
                )
                return kwargs.pop(old_name)
            return None

        parse_only = parse_only or deprecated_argument("parseOnlyThese", "parse_only")
        if parse_only is not None:
            # Issue a warning if we can tell in advance that
            # parse_only will exclude the entire tree.
            if parse_only.excludes_everything:
                warnings.warn(
                    f"The given value for parse_only will exclude everything: {parse_only}",
                    UserWarning,
                    stacklevel=3,
                )

        from_encoding = from_encoding or deprecated_argument(
            "fromEncoding", "from_encoding"
        )

        if from_encoding and isinstance(markup, str):
            warnings.warn(
                "You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored."
            )
            from_encoding = None

        self.element_classes = element_classes or dict()

        # We need this information to track whether or not the builder
        # was specified well enough that we can omit the 'you need to
        # specify a parser' warning.
        original_builder = builder
        original_features = features

        builder_class: Optional[Type[TreeBuilder]] = None
        if isinstance(builder, type):
            # A builder class was passed in; it needs to be instantiated.
            builder_class = builder
            builder = None
        elif builder is None:
            if isinstance(features, str):
                features = [features]
            if features is None or len(features) == 0:
                features = self.DEFAULT_BUILDER_FEATURES
            possible_builder_class = builder_registry.lookup(*features)
            if possible_builder_class is None:
>               raise FeatureNotFound(
                    "Couldn't find a tree builder with the features you "
                    "requested: %s. Do you need to install a parser library?"
                    % ",".join(features)
                )
E               bs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?

/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/bs4/__init__.py:366: FeatureNotFound
______________________ test_parse_and_select_top_results _______________________

    def test_parse_and_select_top_results() -> None:
        scraper = _build_scraper()
        html = (
            "<tbody>"
            "<tr>"
            '<td class="name"><a href="magnet:?a">A</a></td>'
            '<td class="seeds">1</td><td class="leeches">0</td><td class="size">1 GB</td>'
            '<td class="uploader"><a>U</a></td>'
            "</tr>"
            "<tr>"
            '<td class="name"><a href="magnet:?b">B</a></td>'
            '<td class="seeds">5</td><td class="leeches">0</td><td class="size">1 GB</td>'
            '<td class="uploader"><a>U</a></td>'
            "</tr>"
            "<tr>"
            '<td class="name"><a href="magnet:?c">C</a></td>'
            '<td class="seeds">3</td><td class="leeches">0</td><td class="size">1 GB</td>'
            '<td class="uploader"><a>U</a></td>'
            "</tr>"
            "</tbody>"
        )
>       search_area = BeautifulSoup(html, "lxml")
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/services/test_generic_torrent_scraper_topn.py:69:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <[AttributeError("'<class 'bs4.BeautifulSoup'>' object has no attribute 'contents'") raised in repr()] BeautifulSoup object at 0x7ff9863e47d0>
markup = '<tbody><tr><td class="name"><a href="magnet:?a">A</a></td><td class="seeds">1</td><td class="leeches">0</td><td class...lass="seeds">3</td><td class="leeches">0</td><td class="size">1 GB</td><td class="uploader"><a>U</a></td></tr></tbody>'
features = ['lxml'], builder = None, parse_only = None, from_encoding = None
exclude_encodings = None, element_classes = None, kwargs = {}
deprecated_argument = <function BeautifulSoup.__init__.<locals>.deprecated_argument at 0x7ff9862b16c0>
original_builder = None, original_features = 'lxml', builder_class = None
possible_builder_class = None

    def __init__(
        self,
        markup: _IncomingMarkup = "",
        features: Optional[Union[str, Sequence[str]]] = None,
        builder: Optional[Union[TreeBuilder, Type[TreeBuilder]]] = None,
        parse_only: Optional[SoupStrainer] = None,
        from_encoding: Optional[_Encoding] = None,
        exclude_encodings: Optional[_Encodings] = None,
        element_classes: Optional[Dict[Type[PageElement], Type[PageElement]]] = None,
        **kwargs: Any,
    ):
        """Constructor.

        :param markup: A string or a file-like object representing
         markup to be parsed.

        :param features: Desirable features of the parser to be
         used. This may be the name of a specific parser ("lxml",
         "lxml-xml", "html.parser", or "html5lib") or it may be the
         type of markup to be used ("html", "html5", "xml"). It's
         recommended that you name a specific parser, so that
         Beautiful Soup gives you the same results across platforms
         and virtual environments.

        :param builder: A TreeBuilder subclass to instantiate (or
         instance to use) instead of looking one up based on
         `features`. You only need to use this if you've implemented a
         custom TreeBuilder.

        :param parse_only: A SoupStrainer. Only parts of the document
         matching the SoupStrainer will be considered. This is useful
         when parsing part of a document that would otherwise be too
         large to fit into memory.

        :param from_encoding: A string indicating the encoding of the
         document to be parsed. Pass this in if Beautiful Soup is
         guessing wrongly about the document's encoding.

        :param exclude_encodings: A list of strings indicating
         encodings known to be wrong. Pass this in if you don't know
         the document's encoding but you know Beautiful Soup's guess is
         wrong.

        :param element_classes: A dictionary mapping BeautifulSoup
         classes like Tag and NavigableString, to other classes you'd
         like to be instantiated instead as the parse tree is
         built. This is useful for subclassing Tag or NavigableString
         to modify default behavior.

        :param kwargs: For backwards compatibility purposes, the
         constructor accepts certain keyword arguments used in
         Beautiful Soup 3. None of these arguments do anything in
         Beautiful Soup 4; they will result in a warning and then be
         ignored.

         Apart from this, any keyword arguments passed into the
         BeautifulSoup constructor are propagated to the TreeBuilder
         constructor. This makes it possible to configure a
         TreeBuilder by passing in arguments, not just by saying which
         one to use.
        """
        if "convertEntities" in kwargs:
            del kwargs["convertEntities"]
            warnings.warn(
                "BS4 does not respect the convertEntities argument to the "
                "BeautifulSoup constructor. Entities are always converted "
                "to Unicode characters."
            )

        if "markupMassage" in kwargs:
            del kwargs["markupMassage"]
            warnings.warn(
                "BS4 does not respect the markupMassage argument to the "
                "BeautifulSoup constructor. The tree builder is responsible "
                "for any necessary markup massage."
            )

        if "smartQuotesTo" in kwargs:
            del kwargs["smartQuotesTo"]
            warnings.warn(
                "BS4 does not respect the smartQuotesTo argument to the "
                "BeautifulSoup constructor. Smart quotes are always converted "
                "to Unicode characters."
            )

        if "selfClosingTags" in kwargs:
            del kwargs["selfClosingTags"]
            warnings.warn(
                "Beautiful Soup 4 does not respect the selfClosingTags argument to the "
                "BeautifulSoup constructor. The tree builder is responsible "
                "for understanding self-closing tags."
            )

        if "isHTML" in kwargs:
            del kwargs["isHTML"]
            warnings.warn(
                "Beautiful Soup 4 does not respect the isHTML argument to the "
                "BeautifulSoup constructor. Suggest you use "
                "features='lxml' for HTML and features='lxml-xml' for "
                "XML."
            )

        def deprecated_argument(old_name: str, new_name: str) -> Optional[Any]:
            if old_name in kwargs:
                warnings.warn(
                    'The "%s" argument to the BeautifulSoup constructor '
                    'was renamed to "%s" in Beautiful Soup 4.0.0'
                    % (old_name, new_name),
                    DeprecationWarning,
                    stacklevel=3,
                )
                return kwargs.pop(old_name)
            return None

        parse_only = parse_only or deprecated_argument("parseOnlyThese", "parse_only")
        if parse_only is not None:
            # Issue a warning if we can tell in advance that
            # parse_only will exclude the entire tree.
            if parse_only.excludes_everything:
                warnings.warn(
                    f"The given value for parse_only will exclude everything: {parse_only}",
                    UserWarning,
                    stacklevel=3,
                )

        from_encoding = from_encoding or deprecated_argument(
            "fromEncoding", "from_encoding"
        )

        if from_encoding and isinstance(markup, str):
            warnings.warn(
                "You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored."
            )
            from_encoding = None

        self.element_classes = element_classes or dict()

        # We need this information to track whether or not the builder
        # was specified well enough that we can omit the 'you need to
        # specify a parser' warning.
        original_builder = builder
        original_features = features

        builder_class: Optional[Type[TreeBuilder]] = None
        if isinstance(builder, type):
            # A builder class was passed in; it needs to be instantiated.
            builder_class = builder
            builder = None
        elif builder is None:
            if isinstance(features, str):
                features = [features]
            if features is None or len(features) == 0:
                features = self.DEFAULT_BUILDER_FEATURES
            possible_builder_class = builder_registry.lookup(*features)
            if possible_builder_class is None:
>               raise FeatureNotFound(
                    "Couldn't find a tree builder with the features you "
                    "requested: %s. Do you need to install a parser library?"
                    % ",".join(features)
                )
E               bs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?

/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/bs4/__init__.py:366: FeatureNotFound
___________________ test_fetch_episode_title_dedicated_page ____________________

mocker = <pytest_mock.plugin.MockerFixture object at 0x7ff9863d28d0>

    @pytest.mark.asyncio
    async def test_fetch_episode_title_dedicated_page(mocker):
        mock_page = mocker.Mock()
        mock_page.title = "Show"
        mock_page.url = "http://example.com"
        mocker.patch("wikipedia.search", return_value=["Show"])
        mocker.patch("wikipedia.page", return_value=mock_page)
>       mocker.patch(
            "telegram_bot.services.scraping_service._get_page_html",
            return_value=DEDICATED_HTML,
        )

tests/services/test_scraping_service.py:163:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/pytest_mock/plugin.py:448: in __call__
    return self._start_patch(
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/pytest_mock/plugin.py:266: in _start_patch
    mocked: MockType = p.start()
                       ^^^^^^^^^
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1624: in start
    result = self.__enter__()
             ^^^^^^^^^^^^^^^^
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x7ff9863dc740>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'telegram_bot.services.scraping_service' from '/app/telegram_bot/services/scraping_service.py'> does not have the attribute '_get_page_html'

/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1437: AttributeError
______________ test_fetch_episode_title_strips_miniseries_suffix _______________

mocker = <pytest_mock.plugin.MockerFixture object at 0x7ff9863f61e0>

    @pytest.mark.asyncio
    async def test_fetch_episode_title_strips_miniseries_suffix(mocker):
        mock_main_page = mocker.Mock()
        mock_main_page.title = "Show (miniseries)"
        mock_main_page.url = "http://example.com/show"

        mock_list_page = mocker.Mock()
        mock_list_page.url = "http://example.com/list"

        mocker.patch("wikipedia.search", return_value=["Show (miniseries)"])
        page_patch = mocker.patch(
            "wikipedia.page", side_effect=[mock_main_page, mock_list_page]
        )
>       mocker.patch(
            "telegram_bot.services.scraping_service._get_page_html",
            return_value=DEDICATED_HTML,
        )

tests/services/test_scraping_service.py:188:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/pytest_mock/plugin.py:448: in __call__
    return self._start_patch(
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/pytest_mock/plugin.py:266: in _start_patch
    mocked: MockType = p.start()
                       ^^^^^^^^^
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1624: in start
    result = self.__enter__()
             ^^^^^^^^^^^^^^^^
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x7ff9863f7080>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'telegram_bot.services.scraping_service' from '/app/telegram_bot/services/scraping_service.py'> does not have the attribute '_get_page_html'

/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1437: AttributeError
_______________ test_fetch_episode_title_strips_tv_series_suffix _______________

mocker = <pytest_mock.plugin.MockerFixture object at 0x7ff986400d70>

    @pytest.mark.asyncio
    async def test_fetch_episode_title_strips_tv_series_suffix(mocker):
        mock_main_page = mocker.Mock()
        mock_main_page.title = "Show (TV series)"
        mock_main_page.url = "http://example.com/show"

        mock_list_page = mocker.Mock()
        mock_list_page.url = "http://example.com/list"

        mocker.patch("wikipedia.search", return_value=["Show (TV series)"])
        page_patch = mocker.patch(
            "wikipedia.page", side_effect=[mock_main_page, mock_list_page]
        )
>       mocker.patch(
            "telegram_bot.services.scraping_service._get_page_html",
            return_value=DEDICATED_HTML,
        )

tests/services/test_scraping_service.py:215:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/pytest_mock/plugin.py:448: in __call__
    return self._start_patch(
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/pytest_mock/plugin.py:266: in _start_patch
    mocked: MockType = p.start()
                       ^^^^^^^^^
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1624: in start
    result = self.__enter__()
             ^^^^^^^^^^^^^^^^
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x7ff9863b9700>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'telegram_bot.services.scraping_service' from '/app/telegram_bot/services/scraping_service.py'> does not have the attribute '_get_page_html'

/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1437: AttributeError
____________________ test_fetch_episode_title_embedded_page ____________________

mocker = <pytest_mock.plugin.MockerFixture object at 0x7ff9863a5670>

    @pytest.mark.asyncio
    async def test_fetch_episode_title_embedded_page(mocker):
        mock_main_page = mocker.Mock()
        mock_main_page.title = "Show"
        mock_main_page.url = "http://example.com/main"

        mocker.patch("wikipedia.search", return_value=["Show"])
        mocker.patch(
            "wikipedia.page",
            side_effect=[mock_main_page, wikipedia.exceptions.PageError("no list")],
        )
>       mocker.patch(
            "telegram_bot.services.scraping_service._get_page_html",
            return_value=SIMPLE_EMBEDDED_HTML,
        )

tests/services/test_scraping_service.py:240:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/pytest_mock/plugin.py:448: in __call__
    return self._start_patch(
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/pytest_mock/plugin.py:266: in _start_patch
    mocked: MockType = p.start()
                       ^^^^^^^^^
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1624: in start
    result = self.__enter__()
             ^^^^^^^^^^^^^^^^
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x7ff9863a4b30>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'telegram_bot.services.scraping_service' from '/app/telegram_bot/services/scraping_service.py'> does not have the attribute '_get_page_html'

/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1437: AttributeError
______________________ test_fetch_episode_title_not_found ______________________

mocker = <pytest_mock.plugin.MockerFixture object at 0x7ff9863a5ca0>

    @pytest.mark.asyncio
    async def test_fetch_episode_title_not_found(mocker):
        mock_page = mocker.Mock()
        mock_page.url = "http://example.com"
        mocker.patch("wikipedia.search", return_value=["Show"])
        mocker.patch("wikipedia.page", return_value=mock_page)
>       mocker.patch(
            "telegram_bot.services.scraping_service._get_page_html",
            return_value=NO_EPISODE_HTML,
        )

tests/services/test_scraping_service.py:255:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/pytest_mock/plugin.py:448: in __call__
    return self._start_patch(
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/pytest_mock/plugin.py:266: in _start_patch
    mocked: MockType = p.start()
                       ^^^^^^^^^
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1624: in start
    result = self.__enter__()
             ^^^^^^^^^^^^^^^^
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x7ff986403260>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'telegram_bot.services.scraping_service' from '/app/telegram_bot/services/scraping_service.py'> does not have the attribute '_get_page_html'

/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1437: AttributeError
_______________________ test_fetch_season_episode_count ________________________

mocker = <pytest_mock.plugin.MockerFixture object at 0x7ff986400e90>

    @pytest.mark.asyncio
    async def test_fetch_season_episode_count(mocker):
        mock_page = mocker.Mock()
        mock_page.url = "http://example.com"
        mocker.patch("wikipedia.page", return_value=mock_page)
>       mocker.patch(
            "telegram_bot.services.scraping_service._get_page_html",
            return_value=SEASON_OVERVIEW_HTML,
        )

tests/services/test_scraping_service.py:269:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/pytest_mock/plugin.py:448: in __call__
    return self._start_patch(
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/pytest_mock/plugin.py:266: in _start_patch
    mocked: MockType = p.start()
                       ^^^^^^^^^
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1624: in start
    result = self.__enter__()
             ^^^^^^^^^^^^^^^^
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x7ff9863f4a40>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'telegram_bot.services.scraping_service' from '/app/telegram_bot/services/scraping_service.py'> does not have the attribute '_get_page_html'

/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1437: AttributeError
_________ test_fetch_season_episode_count_prefers_titles_over_overview _________

mocker = <pytest_mock.plugin.MockerFixture object at 0x7ff9863f5af0>

    @pytest.mark.asyncio
    async def test_fetch_season_episode_count_prefers_titles_over_overview(mocker):
        mock_page = mocker.Mock()
        mock_page.url = "http://example.com"
        mocker.patch("wikipedia.page", return_value=mock_page)
>       mocker.patch(
            "telegram_bot.services.scraping_service._get_page_html",
            return_value=DEDICATED_WITH_OVERVIEW_ONGOING_HTML,
        )

tests/services/test_scraping_service.py:283:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/pytest_mock/plugin.py:448: in __call__
    return self._start_patch(
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/pytest_mock/plugin.py:266: in _start_patch
    mocked: MockType = p.start()
                       ^^^^^^^^^
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1624: in start
    result = self.__enter__()
             ^^^^^^^^^^^^^^^^
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x7ff9863e4650>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'telegram_bot.services.scraping_service' from '/app/telegram_bot/services/scraping_service.py'> does not have the attribute '_get_page_html'

/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1437: AttributeError
____________ test_fetch_season_episode_count_skips_ongoing_overview ____________

mocker = <pytest_mock.plugin.MockerFixture object at 0x7ff9863d0770>

    @pytest.mark.asyncio
    async def test_fetch_season_episode_count_skips_ongoing_overview(mocker):
        mock_page = mocker.Mock()
        mock_page.url = "http://example.com"
        mocker.patch("wikipedia.page", return_value=mock_page)
>       mocker.patch(
            "telegram_bot.services.scraping_service._get_page_html",
            return_value=OVERVIEW_ONGOING_ONLY_HTML,
        )

tests/services/test_scraping_service.py:298:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/pytest_mock/plugin.py:448: in __call__
    return self._start_patch(
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/pytest_mock/plugin.py:266: in _start_patch
    mocked: MockType = p.start()
                       ^^^^^^^^^
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1624: in start
    result = self.__enter__()
             ^^^^^^^^^^^^^^^^
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x7ff9863d3ef0>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'telegram_bot.services.scraping_service' from '/app/telegram_bot/services/scraping_service.py'> does not have the attribute '_get_page_html'

/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1437: AttributeError
_______________________ test_scrape_1337x_parses_results _______________________

mocker = <pytest_mock.plugin.MockerFixture object at 0x7ff98640e2d0>

    @pytest.mark.asyncio
    async def test_scrape_1337x_parses_results(mocker):
        # This is the response for the initial search results page
        search_html = """
        <table class="table-list"><tbody>
        <tr>
          <td class="name">
            <a href="/cat">Movies</a>
            <a href="/torrent/1/Sample.Movie.2023.1080p.x265/">Sample.Movie.2023.1080p.x265</a>
          </td>
          <td class="seeds">10</td>
          <td class="leeches">0</td>
          <td class="size">1.5 GB</td>
          <td class="uploader"><a>Anonymous</a></td>
        </tr>
        </tbody></table>
        """

        # This is the required second response for the torrent detail page
        detail_html = """
        <div>
          <a class="btn-magnet" href="magnet:?xt=urn:btih:FAKEHASH">Magnet Download</a>
        </div>
        """

        # The mock client now has TWO responses to give, and they will have a default status_code of 200
        responses = [DummyResponse(text=search_html), DummyResponse(text=detail_html)]
        mocker.patch("httpx.AsyncClient", return_value=DummyClient(responses))

        context = Mock()
        context.bot_data = {
            "SEARCH_CONFIG": {
                "preferences": {
                    "movies": {
                        "codecs": {"x265": 5},
                        "resolutions": {"1080p": 3},
                        "uploaders": {"Anonymous": 2},
                    }
                }
            }
        }

>       results = await scraping_service.scrape_1337x(
            "Sample Movie 2023",
            "movie",
            "https://1337x.to/search/{query}/1/",
            context,
            base_query_for_filter="Sample Movie",
        )
E       TypeError: scrape_1337x() takes 2 positional arguments but 4 were given

tests/services/test_scraping_service.py:350: TypeError
_________________________ test_scrape_1337x_no_results _________________________

mocker = <pytest_mock.plugin.MockerFixture object at 0x7ff9863e7d70>

    @pytest.mark.asyncio
    async def test_scrape_1337x_no_results(mocker):
        html = "<html><body>No results</body></html>"
        responses = [DummyResponse(text=html)]
        mocker.patch("httpx.AsyncClient", return_value=DummyClient(responses))

        context = Mock()
        context.bot_data = {
            "SEARCH_CONFIG": {
                "preferences": {
                    "movies": {
                        "codecs": {},
                        "resolutions": {},
                        "uploaders": {},
                    }
                }
            }
        }

>       results = await scraping_service.scrape_1337x(
            "Sample",
            "movie",
            "https://1337x.to/search/{query}/1/",
            context,  # Pass the mock object here
            base_query_for_filter="Sample Movie",
        )
E       TypeError: scrape_1337x() takes 2 positional arguments but 4 were given

tests/services/test_scraping_service.py:383: TypeError
________________________ test_scrape_1337x_fuzzy_filter ________________________

mocker = <pytest_mock.plugin.MockerFixture object at 0x7ff9863c58b0>

    @pytest.mark.asyncio
    async def test_scrape_1337x_fuzzy_filter(mocker):
        """Non-matching titles should be filtered out when fuzzy filter is enabled."""

        search_html = """
        <table class="table-list"><tbody>
        <tr>
          <td class="name">
            <a href="/cat">Movies</a>
            <a href="/torrent/1/Sample.Movie.2023.1080p.x265/">Sample.Movie.2023.1080p.x265</a>
          </td>
          <td class="seeds">10</td>
          <td class="leeches">0</td>
          <td class="size">1.5 GB</td>
          <td class="uploader"><a>Anonymous</a></td>
        </tr>
        <tr>
          <td class="name">
            <a href="/cat">Movies</a>
            <a href="/torrent/2/Unrelated.File.2023.1080p.x265/">Unrelated.File.2023.1080p.x265</a>
          </td>
          <td class="seeds">5</td>
          <td class="leeches">0</td>
          <td class="size">1.0 GB</td>
          <td class="uploader"><a>Anonymous</a></td>
        </tr>
        </tbody></table>
        """

        detail_good = """
        <div><a class="btn-magnet" href="magnet:?xt=urn:btih:GOOD">Magnet</a></div>
        """

        responses = [
            DummyResponse(text=search_html),
            DummyResponse(text=detail_good),
        ]
        client = DummyClient(responses)
        mocker.patch("httpx.AsyncClient", return_value=client)

        context = Mock()
        context.bot_data = {
            "SEARCH_CONFIG": {
                "preferences": {
                    "movies": {
                        "codecs": {"x265": 5},
                        "resolutions": {"1080p": 3},
                        "uploaders": {"Anonymous": 2},
                    }
                }
            }
        }

>       results = await scraping_service.scrape_1337x(
            "Sample Movie 2023",
            "movie",
            "https://1337x.to/search/{query}/1/",
            context,
            base_query_for_filter="Sample Movie",
        )
E       TypeError: scrape_1337x() takes 2 positional arguments but 4 were given

tests/services/test_scraping_service.py:447: TypeError
________________________ test_scrape_1337x_passes_limit ________________________

mocker = <pytest_mock.plugin.MockerFixture object at 0x7ff9863c6ea0>

    @pytest.mark.asyncio
    async def test_scrape_1337x_passes_limit(mocker):
        """scrape_1337x should forward the limit argument to the scraper."""

        mock_search = AsyncMock(return_value=[])
>       mocker.patch.object(scraping_service.GenericTorrentScraper, "search", mock_search)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: module 'telegram_bot.services.scraping_service' has no attribute 'GenericTorrentScraper'

tests/services/test_scraping_service.py:466: AttributeError
________________________ test_scrape_yts_parses_results ________________________

mocker = <pytest_mock.plugin.MockerFixture object at 0x7ff9863cd6d0>

    @pytest.mark.asyncio
    async def test_scrape_yts_parses_results(mocker):
        search_html = """
        <div class="browse-movie-wrap">
          <a class="browse-movie-title" href="https://yts.mx/movies/test-movie">Test Movie</a>
          <div class="browse-movie-year">2023</div>
        </div>
        """
        movie_html = '<div id="movie-info" data-movie-id="1234"></div>'
        api_json = {
            "status": "ok",
            "data": {
                "movie": {
                    "title_long": "Test Movie (2023)",
                    "year": 2023,
                    "torrents": [
                        {
                            "quality": "1080p",
                            "type": "WEB",
                            "size_bytes": 1024**3,
                            "hash": "abcdef",
                            "seeds": 10,
                        }
                    ],
                }
            },
        }
        responses = [
            DummyResponse(text=search_html),
            DummyResponse(text=movie_html),
            DummyResponse(json_data=api_json),
        ]
        mocker.patch("httpx.AsyncClient", return_value=DummyClient(responses))

        context = Mock()
        context.bot_data = {
            "SEARCH_CONFIG": {
                "preferences": {
                    "movies": {
                        "codecs": {"x264": 5},
                        "resolutions": {"1080p": 3},
                        "uploaders": {"YTS": 2},
                    }
                }
            }
        }

>       results = await scraping_service.scrape_yts(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
            "Test Movie",
            "movie",
            "https://yts.mx/browse-movies/{query}",
            context,  # Pass the mock object here
            year="2023",
            resolution="1080p",
        )
E       AttributeError: module 'telegram_bot.services.scraping_service' has no attribute 'scrape_yts'

tests/services/test_scraping_service.py:528: AttributeError
________________ test_scrape_yts_retries_on_validation_failure _________________

caplog = <_pytest.logging.LogCaptureFixture object at 0x7ff9863d0860>
mocker = <pytest_mock.plugin.MockerFixture object at 0x7ff9863d3590>

    @pytest.mark.asyncio
    async def test_scrape_yts_retries_on_validation_failure(caplog, mocker):
        """YTS scraper retries when API validation fails."""
        search_html = """
        <div class="browse-movie-wrap">
          <a class="browse-movie-title" href="https://yts.mx/movies/test-movie">Test Movie</a>
          <div class="browse-movie-year">2023</div>
        </div>
        """
        movie_html = '<div id="movie-info" data-movie-id="1234"></div>'
        bad_api_json = {
            "status": "ok",
            "data": {
                "movie": {
                    "title_long": "Test Movie (2023)",
                    "year": 2023,
                    "torrents": [],  # Missing torrents triggers retry
                }
            },
        }
        good_api_json = {
            "status": "ok",
            "data": {
                "movie": {
                    "title_long": "Test Movie (2023)",
                    "year": 2023,
                    "torrents": [
                        {
                            "quality": "1080p",
                            "type": "WEB",
                            "size_bytes": 1024**3,
                            "hash": "abcdef",
                            "seeds": 10,
                        }
                    ],
                }
            },
        }
        responses = [
            DummyResponse(text=search_html),
            DummyResponse(text=movie_html),
            DummyResponse(json_data=bad_api_json),
            DummyResponse(json_data=good_api_json),
        ]
        mocker.patch("httpx.AsyncClient", return_value=DummyClient(responses))
        mocker.patch("asyncio.sleep", new=AsyncMock())

        context = Mock()
        context.bot_data = {
            "SEARCH_CONFIG": {
                "preferences": {
                    "movies": {
                        "codecs": {"x264": 5},
                        "resolutions": {"1080p": 3},
                        "uploaders": {"YTS": 2},
                    }
                }
            }
        }

        with caplog.at_level(logging.DEBUG):
>           results = await scraping_service.scrape_yts(
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
                "Test Movie",
                "movie",
                "https://yts.mx/browse-movies/{query}",
                context,
                year="2023",
                resolution="1080p",
            )
E           AttributeError: module 'telegram_bot.services.scraping_service' has no attribute 'scrape_yts'

tests/services/test_scraping_service.py:603: AttributeError
_____________ test_scrape_yts_paginates_browse_pages_to_find_year ______________

mocker = <pytest_mock.plugin.MockerFixture object at 0x7ff9863f6120>

    @pytest.mark.asyncio
    async def test_scrape_yts_paginates_browse_pages_to_find_year(mocker):
        """When page 1 has no matching year, the scraper paginates to find older films."""
        # Page 1: no matching movies for the given year
        search_html_p1 = """
        <div class="browse-movie-wrap">
          <a class="browse-movie-title" href="https://yts.mx/movies/alien-xyz">Alien Something</a>
          <div class="browse-movie-year">2003</div>
        </div>
        """
        # Page 2: contains the correct 1979 entry
        search_html_p2 = """
        <div class="browse-movie-wrap">
          <a class="browse-movie-title" href="https://yts.mx/movies/alien-1979">Alien</a>
          <div class="browse-movie-year">1979</div>
        </div>
        """
        movie_html = '<div id="movie-info" data-movie-id="1234"></div>'
        api_json = {
            "status": "ok",
            "data": {
                "movie": {
                    "title_long": "Alien (1979)",
                    "year": 1979,
                    "torrents": [
                        {
                            "quality": "1080p",
                            "type": "WEB",
                            "size_bytes": 1024**3,
                            "hash": "abcdef",
                            "seeds": 10,
                        }
                    ],
                }
            },
        }

        responses = [
            DummyResponse(text=search_html_p1),  # browse page 1
            DummyResponse(text=search_html_p2),  # browse page 2
            DummyResponse(text=movie_html),  # movie page
            DummyResponse(json_data=api_json),  # details API
        ]
        mocker.patch("httpx.AsyncClient", return_value=DummyClient(responses))

        context = Mock()
        context.bot_data = {
            "SEARCH_CONFIG": {
                "preferences": {
                    "movies": {
                        "codecs": {"x264": 5},
                        "resolutions": {"1080p": 3},
                        "uploaders": {"YTS": 2},
                    }
                }
            }
        }

>       results = await scraping_service.scrape_yts(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
            "Alien",
            "movie",
            "https://yts.mx/browse-movies/{query}",
            context,
            year="1979",
            resolution="1080p",
        )
E       AttributeError: module 'telegram_bot.services.scraping_service' has no attribute 'scrape_yts'

tests/services/test_scraping_service.py:675: AttributeError
_________________ test_scrape_yts_api_fallback_relaxes_quality _________________

mocker = <pytest_mock.plugin.MockerFixture object at 0x7ff986411c70>

    @pytest.mark.asyncio
    async def test_scrape_yts_api_fallback_relaxes_quality(mocker):
        """API fallback tries again without quality when the first pass returns 0."""
        # No browse matches -> triggers API fallback
        search_html = """
        <div class="other"></div>
        """
        # Attempt 1 (year+quality): 0 movies
        api_empty = {"status": "ok", "data": {"movie_count": 0}}
        # Attempt 2 (year only): has one movie with 1080p torrent
        api_with_movie = {
            "status": "ok",
            "data": {
                "movies": [
                    {
                        "title_long": "Test Movie (1979)",
                        "year": 1979,
                        "torrents": [
                            {
                                "quality": "1080p",
                                "type": "WEB",
                                "size_bytes": 1024**3,
                                "hash": "abcdef",
                                "seeds": 7,
                            }
                        ],
                    }
                ]
            },
        }

        responses = [
            DummyResponse(text=search_html),  # browse page 1 (no choices)
            DummyResponse(text=search_html),  # browse page 2 (still no choices)
            DummyResponse(text=search_html),  # browse page 3
            DummyResponse(text=search_html),  # browse page 4
            DummyResponse(text=search_html),  # browse page 5
            DummyResponse(json_data=api_empty),  # API attempt 1 (year+quality)
            DummyResponse(json_data=api_with_movie),  # API attempt 2 (year only)
        ]
        mocker.patch("httpx.AsyncClient", return_value=DummyClient(responses))

        context = Mock()
        context.bot_data = {
            "SEARCH_CONFIG": {
                "preferences": {
                    "movies": {
                        "codecs": {"x264": 5},
                        "resolutions": {"1080p": 3},
                        "uploaders": {"YTS": 2},
                    }
                }
            }
        }

>       results = await scraping_service.scrape_yts(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
            "Test Movie",
            "movie",
            "https://yts.mx/browse-movies/{query}",
            context,
            year="1979",
            resolution="1080p",
        )
E       AttributeError: module 'telegram_bot.services.scraping_service' has no attribute 'scrape_yts'

tests/services/test_scraping_service.py:744: AttributeError
__________________ test_scrape_yts_api_fallback_relaxes_year ___________________

mocker = <pytest_mock.plugin.MockerFixture object at 0x7ff986400560>

    @pytest.mark.asyncio
    async def test_scrape_yts_api_fallback_relaxes_year(mocker):
        """API fallback eventually drops year param and filters locally by year."""
        # No browse matches -> triggers API fallback
        search_html = '<div class="other"></div>'
        api_empty = {"status": "ok", "data": {"movie_count": 0}}
        # Attempt 3 (no year param): include multiple years, only target year remains
        api_all_years = {
            "status": "ok",
            "data": {
                "movies": [
                    {
                        "title_long": "Alien (1979)",
                        "year": 1979,
                        "torrents": [
                            {
                                "quality": "1080p",
                                "type": "WEB",
                                "size_bytes": 1024**3,
                                "hash": "abcd11",
                                "seeds": 5,
                            }
                        ],
                    },
                    {
                        "title_long": "Alien (2012)",
                        "year": 2012,
                        "torrents": [
                            {
                                "quality": "1080p",
                                "type": "WEB",
                                "size_bytes": 1024**3,
                                "hash": "efgh22",
                                "seeds": 9,
                            }
                        ],
                    },
                ]
            },
        }

        responses = [
            DummyResponse(text=search_html),  # browse page 1
            DummyResponse(text=search_html),  # browse page 2
            DummyResponse(text=search_html),  # browse page 3
            DummyResponse(text=search_html),  # browse page 4
            DummyResponse(text=search_html),  # browse page 5
            DummyResponse(json_data=api_empty),  # API attempt 1 (year+quality)
            DummyResponse(json_data=api_empty),  # API attempt 2 (year only)
            DummyResponse(json_data=api_all_years),  # API attempt 3 (no year param)
        ]
        mocker.patch("httpx.AsyncClient", return_value=DummyClient(responses))

        context = Mock()
        context.bot_data = {
            "SEARCH_CONFIG": {
                "preferences": {
                    "movies": {
                        "codecs": {"x264": 5},
                        "resolutions": {"1080p": 3},
                        "uploaders": {"YTS": 2},
                    }
                }
            }
        }

>       results = await scraping_service.scrape_yts(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
            "Alien",
            "movie",
            "https://yts.mx/browse-movies/{query}",
            context,
            year="1979",
            resolution="1080p",
        )
E       AttributeError: module 'telegram_bot.services.scraping_service' has no attribute 'scrape_yts'

tests/services/test_scraping_service.py:824: AttributeError
_______________ test_scrape_yts_token_gate_avoids_near_homonyms ________________

mocker = <pytest_mock.plugin.MockerFixture object at 0x7ff9863b8b90>

    @pytest.mark.asyncio
    async def test_scrape_yts_token_gate_avoids_near_homonyms(mocker):
        """With a year present, token gate avoids false matches like 'The Dunes' for 'Dune'."""
        # Page 1 contains 'The Dunes' (fails token gate), then pages 2-5 empty -> fallback
        browse_dunes = """
        <div class="browse-movie-wrap">
          <a class="browse-movie-title" href="https://yts.mx/movies/the-dunes-1979">The Dunes</a>
          <div class="browse-movie-year">1979</div>
        </div>
        """
        browse_empty = '<div class="other"></div>'

        api_with_movie = {
            "status": "ok",
            "data": {
                "movies": [
                    {
                        "title_long": "Dune (1979)",
                        "year": 1979,
                        "torrents": [
                            {
                                "quality": "1080p",
                                "type": "WEB",
                                "size_bytes": 1024**3,
                                "hash": "aaaaaa",
                                "seeds": 3,
                            }
                        ],
                    }
                ]
            },
        }

        responses = [
            DummyResponse(text=browse_dunes),  # page 1 (gated out)
            DummyResponse(text=browse_empty),  # page 2
            DummyResponse(text=browse_empty),  # page 3
            DummyResponse(text=browse_empty),  # page 4
            DummyResponse(text=browse_empty),  # page 5
            DummyResponse(json_data=api_with_movie),  # API fallback
        ]
        mocker.patch("httpx.AsyncClient", return_value=DummyClient(responses))

        context = Mock()
        context.bot_data = {
            "SEARCH_CONFIG": {
                "preferences": {
                    "movies": {
                        "codecs": {"x264": 5},
                        "resolutions": {"1080p": 3},
                        "uploaders": {"YTS": 2},
                    }
                }
            }
        }

>       results = await scraping_service.scrape_yts(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
            "Dune",
            "movie",
            "https://yts.mx/browse-movies/{query}",
            context,
            year="1979",
            resolution="1080p",
        )
E       AttributeError: module 'telegram_bot.services.scraping_service' has no attribute 'scrape_yts'

tests/services/test_scraping_service.py:898: AttributeError
____________________ test_strategy_find_direct_links_magnet ____________________

    def test_strategy_find_direct_links_magnet():
        html = '<a href="magnet:?xt=urn:btih:123">Magnet</a>'
>       soup = BeautifulSoup(html, "lxml")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/services/test_scraping_service.py:913:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <[AttributeError("'<class 'bs4.BeautifulSoup'>' object has no attribute 'contents'") raised in repr()] BeautifulSoup object at 0x7ff9863a4650>
markup = '<a href="magnet:?xt=urn:btih:123">Magnet</a>', features = ['lxml']
builder = None, parse_only = None, from_encoding = None
exclude_encodings = None, element_classes = None, kwargs = {}
deprecated_argument = <function BeautifulSoup.__init__.<locals>.deprecated_argument at 0x7ff9862a9620>
original_builder = None, original_features = 'lxml', builder_class = None
possible_builder_class = None

    def __init__(
        self,
        markup: _IncomingMarkup = "",
        features: Optional[Union[str, Sequence[str]]] = None,
        builder: Optional[Union[TreeBuilder, Type[TreeBuilder]]] = None,
        parse_only: Optional[SoupStrainer] = None,
        from_encoding: Optional[_Encoding] = None,
        exclude_encodings: Optional[_Encodings] = None,
        element_classes: Optional[Dict[Type[PageElement], Type[PageElement]]] = None,
        **kwargs: Any,
    ):
        """Constructor.

        :param markup: A string or a file-like object representing
         markup to be parsed.

        :param features: Desirable features of the parser to be
         used. This may be the name of a specific parser ("lxml",
         "lxml-xml", "html.parser", or "html5lib") or it may be the
         type of markup to be used ("html", "html5", "xml"). It's
         recommended that you name a specific parser, so that
         Beautiful Soup gives you the same results across platforms
         and virtual environments.

        :param builder: A TreeBuilder subclass to instantiate (or
         instance to use) instead of looking one up based on
         `features`. You only need to use this if you've implemented a
         custom TreeBuilder.

        :param parse_only: A SoupStrainer. Only parts of the document
         matching the SoupStrainer will be considered. This is useful
         when parsing part of a document that would otherwise be too
         large to fit into memory.

        :param from_encoding: A string indicating the encoding of the
         document to be parsed. Pass this in if Beautiful Soup is
         guessing wrongly about the document's encoding.

        :param exclude_encodings: A list of strings indicating
         encodings known to be wrong. Pass this in if you don't know
         the document's encoding but you know Beautiful Soup's guess is
         wrong.

        :param element_classes: A dictionary mapping BeautifulSoup
         classes like Tag and NavigableString, to other classes you'd
         like to be instantiated instead as the parse tree is
         built. This is useful for subclassing Tag or NavigableString
         to modify default behavior.

        :param kwargs: For backwards compatibility purposes, the
         constructor accepts certain keyword arguments used in
         Beautiful Soup 3. None of these arguments do anything in
         Beautiful Soup 4; they will result in a warning and then be
         ignored.

         Apart from this, any keyword arguments passed into the
         BeautifulSoup constructor are propagated to the TreeBuilder
         constructor. This makes it possible to configure a
         TreeBuilder by passing in arguments, not just by saying which
         one to use.
        """
        if "convertEntities" in kwargs:
            del kwargs["convertEntities"]
            warnings.warn(
                "BS4 does not respect the convertEntities argument to the "
                "BeautifulSoup constructor. Entities are always converted "
                "to Unicode characters."
            )

        if "markupMassage" in kwargs:
            del kwargs["markupMassage"]
            warnings.warn(
                "BS4 does not respect the markupMassage argument to the "
                "BeautifulSoup constructor. The tree builder is responsible "
                "for any necessary markup massage."
            )

        if "smartQuotesTo" in kwargs:
            del kwargs["smartQuotesTo"]
            warnings.warn(
                "BS4 does not respect the smartQuotesTo argument to the "
                "BeautifulSoup constructor. Smart quotes are always converted "
                "to Unicode characters."
            )

        if "selfClosingTags" in kwargs:
            del kwargs["selfClosingTags"]
            warnings.warn(
                "Beautiful Soup 4 does not respect the selfClosingTags argument to the "
                "BeautifulSoup constructor. The tree builder is responsible "
                "for understanding self-closing tags."
            )

        if "isHTML" in kwargs:
            del kwargs["isHTML"]
            warnings.warn(
                "Beautiful Soup 4 does not respect the isHTML argument to the "
                "BeautifulSoup constructor. Suggest you use "
                "features='lxml' for HTML and features='lxml-xml' for "
                "XML."
            )

        def deprecated_argument(old_name: str, new_name: str) -> Optional[Any]:
            if old_name in kwargs:
                warnings.warn(
                    'The "%s" argument to the BeautifulSoup constructor '
                    'was renamed to "%s" in Beautiful Soup 4.0.0'
                    % (old_name, new_name),
                    DeprecationWarning,
                    stacklevel=3,
                )
                return kwargs.pop(old_name)
            return None

        parse_only = parse_only or deprecated_argument("parseOnlyThese", "parse_only")
        if parse_only is not None:
            # Issue a warning if we can tell in advance that
            # parse_only will exclude the entire tree.
            if parse_only.excludes_everything:
                warnings.warn(
                    f"The given value for parse_only will exclude everything: {parse_only}",
                    UserWarning,
                    stacklevel=3,
                )

        from_encoding = from_encoding or deprecated_argument(
            "fromEncoding", "from_encoding"
        )

        if from_encoding and isinstance(markup, str):
            warnings.warn(
                "You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored."
            )
            from_encoding = None

        self.element_classes = element_classes or dict()

        # We need this information to track whether or not the builder
        # was specified well enough that we can omit the 'you need to
        # specify a parser' warning.
        original_builder = builder
        original_features = features

        builder_class: Optional[Type[TreeBuilder]] = None
        if isinstance(builder, type):
            # A builder class was passed in; it needs to be instantiated.
            builder_class = builder
            builder = None
        elif builder is None:
            if isinstance(features, str):
                features = [features]
            if features is None or len(features) == 0:
                features = self.DEFAULT_BUILDER_FEATURES
            possible_builder_class = builder_registry.lookup(*features)
            if possible_builder_class is None:
>               raise FeatureNotFound(
                    "Couldn't find a tree builder with the features you "
                    "requested: %s. Do you need to install a parser library?"
                    % ",".join(features)
                )
E               bs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?

/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/bs4/__init__.py:366: FeatureNotFound
___________________ test_strategy_find_direct_links_torrent ____________________

    def test_strategy_find_direct_links_torrent():
        html = '<a href="https://example.com/file.torrent">Download</a>'
>       soup = BeautifulSoup(html, "lxml")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/services/test_scraping_service.py:920:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <[AttributeError("'<class 'bs4.BeautifulSoup'>' object has no attribute 'contents'") raised in repr()] BeautifulSoup object at 0x7ff9863bbaa0>
markup = '<a href="https://example.com/file.torrent">Download</a>'
features = ['lxml'], builder = None, parse_only = None, from_encoding = None
exclude_encodings = None, element_classes = None, kwargs = {}
deprecated_argument = <function BeautifulSoup.__init__.<locals>.deprecated_argument at 0x7ff9862b3a60>
original_builder = None, original_features = 'lxml', builder_class = None
possible_builder_class = None

    def __init__(
        self,
        markup: _IncomingMarkup = "",
        features: Optional[Union[str, Sequence[str]]] = None,
        builder: Optional[Union[TreeBuilder, Type[TreeBuilder]]] = None,
        parse_only: Optional[SoupStrainer] = None,
        from_encoding: Optional[_Encoding] = None,
        exclude_encodings: Optional[_Encodings] = None,
        element_classes: Optional[Dict[Type[PageElement], Type[PageElement]]] = None,
        **kwargs: Any,
    ):
        """Constructor.

        :param markup: A string or a file-like object representing
         markup to be parsed.

        :param features: Desirable features of the parser to be
         used. This may be the name of a specific parser ("lxml",
         "lxml-xml", "html.parser", or "html5lib") or it may be the
         type of markup to be used ("html", "html5", "xml"). It's
         recommended that you name a specific parser, so that
         Beautiful Soup gives you the same results across platforms
         and virtual environments.

        :param builder: A TreeBuilder subclass to instantiate (or
         instance to use) instead of looking one up based on
         `features`. You only need to use this if you've implemented a
         custom TreeBuilder.

        :param parse_only: A SoupStrainer. Only parts of the document
         matching the SoupStrainer will be considered. This is useful
         when parsing part of a document that would otherwise be too
         large to fit into memory.

        :param from_encoding: A string indicating the encoding of the
         document to be parsed. Pass this in if Beautiful Soup is
         guessing wrongly about the document's encoding.

        :param exclude_encodings: A list of strings indicating
         encodings known to be wrong. Pass this in if you don't know
         the document's encoding but you know Beautiful Soup's guess is
         wrong.

        :param element_classes: A dictionary mapping BeautifulSoup
         classes like Tag and NavigableString, to other classes you'd
         like to be instantiated instead as the parse tree is
         built. This is useful for subclassing Tag or NavigableString
         to modify default behavior.

        :param kwargs: For backwards compatibility purposes, the
         constructor accepts certain keyword arguments used in
         Beautiful Soup 3. None of these arguments do anything in
         Beautiful Soup 4; they will result in a warning and then be
         ignored.

         Apart from this, any keyword arguments passed into the
         BeautifulSoup constructor are propagated to the TreeBuilder
         constructor. This makes it possible to configure a
         TreeBuilder by passing in arguments, not just by saying which
         one to use.
        """
        if "convertEntities" in kwargs:
            del kwargs["convertEntities"]
            warnings.warn(
                "BS4 does not respect the convertEntities argument to the "
                "BeautifulSoup constructor. Entities are always converted "
                "to Unicode characters."
            )

        if "markupMassage" in kwargs:
            del kwargs["markupMassage"]
            warnings.warn(
                "BS4 does not respect the markupMassage argument to the "
                "BeautifulSoup constructor. The tree builder is responsible "
                "for any necessary markup massage."
            )

        if "smartQuotesTo" in kwargs:
            del kwargs["smartQuotesTo"]
            warnings.warn(
                "BS4 does not respect the smartQuotesTo argument to the "
                "BeautifulSoup constructor. Smart quotes are always converted "
                "to Unicode characters."
            )

        if "selfClosingTags" in kwargs:
            del kwargs["selfClosingTags"]
            warnings.warn(
                "Beautiful Soup 4 does not respect the selfClosingTags argument to the "
                "BeautifulSoup constructor. The tree builder is responsible "
                "for understanding self-closing tags."
            )

        if "isHTML" in kwargs:
            del kwargs["isHTML"]
            warnings.warn(
                "Beautiful Soup 4 does not respect the isHTML argument to the "
                "BeautifulSoup constructor. Suggest you use "
                "features='lxml' for HTML and features='lxml-xml' for "
                "XML."
            )

        def deprecated_argument(old_name: str, new_name: str) -> Optional[Any]:
            if old_name in kwargs:
                warnings.warn(
                    'The "%s" argument to the BeautifulSoup constructor '
                    'was renamed to "%s" in Beautiful Soup 4.0.0'
                    % (old_name, new_name),
                    DeprecationWarning,
                    stacklevel=3,
                )
                return kwargs.pop(old_name)
            return None

        parse_only = parse_only or deprecated_argument("parseOnlyThese", "parse_only")
        if parse_only is not None:
            # Issue a warning if we can tell in advance that
            # parse_only will exclude the entire tree.
            if parse_only.excludes_everything:
                warnings.warn(
                    f"The given value for parse_only will exclude everything: {parse_only}",
                    UserWarning,
                    stacklevel=3,
                )

        from_encoding = from_encoding or deprecated_argument(
            "fromEncoding", "from_encoding"
        )

        if from_encoding and isinstance(markup, str):
            warnings.warn(
                "You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored."
            )
            from_encoding = None

        self.element_classes = element_classes or dict()

        # We need this information to track whether or not the builder
        # was specified well enough that we can omit the 'you need to
        # specify a parser' warning.
        original_builder = builder
        original_features = features

        builder_class: Optional[Type[TreeBuilder]] = None
        if isinstance(builder, type):
            # A builder class was passed in; it needs to be instantiated.
            builder_class = builder
            builder = None
        elif builder is None:
            if isinstance(features, str):
                features = [features]
            if features is None or len(features) == 0:
                features = self.DEFAULT_BUILDER_FEATURES
            possible_builder_class = builder_registry.lookup(*features)
            if possible_builder_class is None:
>               raise FeatureNotFound(
                    "Couldn't find a tree builder with the features you "
                    "requested: %s. Do you need to install a parser library?"
                    % ",".join(features)
                )
E               bs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?

/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/bs4/__init__.py:366: FeatureNotFound
_____________________ test_strategy_find_direct_links_none _____________________

    def test_strategy_find_direct_links_none():
        html = '<a href="/other">Link</a>'
>       soup = BeautifulSoup(html, "lxml")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/services/test_scraping_service.py:927:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <[AttributeError("'<class 'bs4.BeautifulSoup'>' object has no attribute 'contents'") raised in repr()] BeautifulSoup object at 0x7ff9863b8650>
markup = '<a href="/other">Link</a>', features = ['lxml'], builder = None
parse_only = None, from_encoding = None, exclude_encodings = None
element_classes = None, kwargs = {}
deprecated_argument = <function BeautifulSoup.__init__.<locals>.deprecated_argument at 0x7ff9862b2ac0>
original_builder = None, original_features = 'lxml', builder_class = None
possible_builder_class = None

    def __init__(
        self,
        markup: _IncomingMarkup = "",
        features: Optional[Union[str, Sequence[str]]] = None,
        builder: Optional[Union[TreeBuilder, Type[TreeBuilder]]] = None,
        parse_only: Optional[SoupStrainer] = None,
        from_encoding: Optional[_Encoding] = None,
        exclude_encodings: Optional[_Encodings] = None,
        element_classes: Optional[Dict[Type[PageElement], Type[PageElement]]] = None,
        **kwargs: Any,
    ):
        """Constructor.

        :param markup: A string or a file-like object representing
         markup to be parsed.

        :param features: Desirable features of the parser to be
         used. This may be the name of a specific parser ("lxml",
         "lxml-xml", "html.parser", or "html5lib") or it may be the
         type of markup to be used ("html", "html5", "xml"). It's
         recommended that you name a specific parser, so that
         Beautiful Soup gives you the same results across platforms
         and virtual environments.

        :param builder: A TreeBuilder subclass to instantiate (or
         instance to use) instead of looking one up based on
         `features`. You only need to use this if you've implemented a
         custom TreeBuilder.

        :param parse_only: A SoupStrainer. Only parts of the document
         matching the SoupStrainer will be considered. This is useful
         when parsing part of a document that would otherwise be too
         large to fit into memory.

        :param from_encoding: A string indicating the encoding of the
         document to be parsed. Pass this in if Beautiful Soup is
         guessing wrongly about the document's encoding.

        :param exclude_encodings: A list of strings indicating
         encodings known to be wrong. Pass this in if you don't know
         the document's encoding but you know Beautiful Soup's guess is
         wrong.

        :param element_classes: A dictionary mapping BeautifulSoup
         classes like Tag and NavigableString, to other classes you'd
         like to be instantiated instead as the parse tree is
         built. This is useful for subclassing Tag or NavigableString
         to modify default behavior.

        :param kwargs: For backwards compatibility purposes, the
         constructor accepts certain keyword arguments used in
         Beautiful Soup 3. None of these arguments do anything in
         Beautiful Soup 4; they will result in a warning and then be
         ignored.

         Apart from this, any keyword arguments passed into the
         BeautifulSoup constructor are propagated to the TreeBuilder
         constructor. This makes it possible to configure a
         TreeBuilder by passing in arguments, not just by saying which
         one to use.
        """
        if "convertEntities" in kwargs:
            del kwargs["convertEntities"]
            warnings.warn(
                "BS4 does not respect the convertEntities argument to the "
                "BeautifulSoup constructor. Entities are always converted "
                "to Unicode characters."
            )

        if "markupMassage" in kwargs:
            del kwargs["markupMassage"]
            warnings.warn(
                "BS4 does not respect the markupMassage argument to the "
                "BeautifulSoup constructor. The tree builder is responsible "
                "for any necessary markup massage."
            )

        if "smartQuotesTo" in kwargs:
            del kwargs["smartQuotesTo"]
            warnings.warn(
                "BS4 does not respect the smartQuotesTo argument to the "
                "BeautifulSoup constructor. Smart quotes are always converted "
                "to Unicode characters."
            )

        if "selfClosingTags" in kwargs:
            del kwargs["selfClosingTags"]
            warnings.warn(
                "Beautiful Soup 4 does not respect the selfClosingTags argument to the "
                "BeautifulSoup constructor. The tree builder is responsible "
                "for understanding self-closing tags."
            )

        if "isHTML" in kwargs:
            del kwargs["isHTML"]
            warnings.warn(
                "Beautiful Soup 4 does not respect the isHTML argument to the "
                "BeautifulSoup constructor. Suggest you use "
                "features='lxml' for HTML and features='lxml-xml' for "
                "XML."
            )

        def deprecated_argument(old_name: str, new_name: str) -> Optional[Any]:
            if old_name in kwargs:
                warnings.warn(
                    'The "%s" argument to the BeautifulSoup constructor '
                    'was renamed to "%s" in Beautiful Soup 4.0.0'
                    % (old_name, new_name),
                    DeprecationWarning,
                    stacklevel=3,
                )
                return kwargs.pop(old_name)
            return None

        parse_only = parse_only or deprecated_argument("parseOnlyThese", "parse_only")
        if parse_only is not None:
            # Issue a warning if we can tell in advance that
            # parse_only will exclude the entire tree.
            if parse_only.excludes_everything:
                warnings.warn(
                    f"The given value for parse_only will exclude everything: {parse_only}",
                    UserWarning,
                    stacklevel=3,
                )

        from_encoding = from_encoding or deprecated_argument(
            "fromEncoding", "from_encoding"
        )

        if from_encoding and isinstance(markup, str):
            warnings.warn(
                "You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored."
            )
            from_encoding = None

        self.element_classes = element_classes or dict()

        # We need this information to track whether or not the builder
        # was specified well enough that we can omit the 'you need to
        # specify a parser' warning.
        original_builder = builder
        original_features = features

        builder_class: Optional[Type[TreeBuilder]] = None
        if isinstance(builder, type):
            # A builder class was passed in; it needs to be instantiated.
            builder_class = builder
            builder = None
        elif builder is None:
            if isinstance(features, str):
                features = [features]
            if features is None or len(features) == 0:
                features = self.DEFAULT_BUILDER_FEATURES
            possible_builder_class = builder_registry.lookup(*features)
            if possible_builder_class is None:
>               raise FeatureNotFound(
                    "Couldn't find a tree builder with the features you "
                    "requested: %s. Do you need to install a parser library?"
                    % ",".join(features)
                )
E               bs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?

/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/bs4/__init__.py:366: FeatureNotFound
___________________ test_strategy_contextual_search_keyword ____________________

    def test_strategy_contextual_search_keyword():
        html = '<a href="/download/123">Download Torrent</a>'
>       soup = BeautifulSoup(html, "lxml")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/services/test_scraping_service.py:934:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <[AttributeError("'<class 'bs4.BeautifulSoup'>' object has no attribute 'contents'") raised in repr()] BeautifulSoup object at 0x7ff9863b8dd0>
markup = '<a href="/download/123">Download Torrent</a>', features = ['lxml']
builder = None, parse_only = None, from_encoding = None
exclude_encodings = None, element_classes = None, kwargs = {}
deprecated_argument = <function BeautifulSoup.__init__.<locals>.deprecated_argument at 0x7ff9862b3b00>
original_builder = None, original_features = 'lxml', builder_class = None
possible_builder_class = None

    def __init__(
        self,
        markup: _IncomingMarkup = "",
        features: Optional[Union[str, Sequence[str]]] = None,
        builder: Optional[Union[TreeBuilder, Type[TreeBuilder]]] = None,
        parse_only: Optional[SoupStrainer] = None,
        from_encoding: Optional[_Encoding] = None,
        exclude_encodings: Optional[_Encodings] = None,
        element_classes: Optional[Dict[Type[PageElement], Type[PageElement]]] = None,
        **kwargs: Any,
    ):
        """Constructor.

        :param markup: A string or a file-like object representing
         markup to be parsed.

        :param features: Desirable features of the parser to be
         used. This may be the name of a specific parser ("lxml",
         "lxml-xml", "html.parser", or "html5lib") or it may be the
         type of markup to be used ("html", "html5", "xml"). It's
         recommended that you name a specific parser, so that
         Beautiful Soup gives you the same results across platforms
         and virtual environments.

        :param builder: A TreeBuilder subclass to instantiate (or
         instance to use) instead of looking one up based on
         `features`. You only need to use this if you've implemented a
         custom TreeBuilder.

        :param parse_only: A SoupStrainer. Only parts of the document
         matching the SoupStrainer will be considered. This is useful
         when parsing part of a document that would otherwise be too
         large to fit into memory.

        :param from_encoding: A string indicating the encoding of the
         document to be parsed. Pass this in if Beautiful Soup is
         guessing wrongly about the document's encoding.

        :param exclude_encodings: A list of strings indicating
         encodings known to be wrong. Pass this in if you don't know
         the document's encoding but you know Beautiful Soup's guess is
         wrong.

        :param element_classes: A dictionary mapping BeautifulSoup
         classes like Tag and NavigableString, to other classes you'd
         like to be instantiated instead as the parse tree is
         built. This is useful for subclassing Tag or NavigableString
         to modify default behavior.

        :param kwargs: For backwards compatibility purposes, the
         constructor accepts certain keyword arguments used in
         Beautiful Soup 3. None of these arguments do anything in
         Beautiful Soup 4; they will result in a warning and then be
         ignored.

         Apart from this, any keyword arguments passed into the
         BeautifulSoup constructor are propagated to the TreeBuilder
         constructor. This makes it possible to configure a
         TreeBuilder by passing in arguments, not just by saying which
         one to use.
        """
        if "convertEntities" in kwargs:
            del kwargs["convertEntities"]
            warnings.warn(
                "BS4 does not respect the convertEntities argument to the "
                "BeautifulSoup constructor. Entities are always converted "
                "to Unicode characters."
            )

        if "markupMassage" in kwargs:
            del kwargs["markupMassage"]
            warnings.warn(
                "BS4 does not respect the markupMassage argument to the "
                "BeautifulSoup constructor. The tree builder is responsible "
                "for any necessary markup massage."
            )

        if "smartQuotesTo" in kwargs:
            del kwargs["smartQuotesTo"]
            warnings.warn(
                "BS4 does not respect the smartQuotesTo argument to the "
                "BeautifulSoup constructor. Smart quotes are always converted "
                "to Unicode characters."
            )

        if "selfClosingTags" in kwargs:
            del kwargs["selfClosingTags"]
            warnings.warn(
                "Beautiful Soup 4 does not respect the selfClosingTags argument to the "
                "BeautifulSoup constructor. The tree builder is responsible "
                "for understanding self-closing tags."
            )

        if "isHTML" in kwargs:
            del kwargs["isHTML"]
            warnings.warn(
                "Beautiful Soup 4 does not respect the isHTML argument to the "
                "BeautifulSoup constructor. Suggest you use "
                "features='lxml' for HTML and features='lxml-xml' for "
                "XML."
            )

        def deprecated_argument(old_name: str, new_name: str) -> Optional[Any]:
            if old_name in kwargs:
                warnings.warn(
                    'The "%s" argument to the BeautifulSoup constructor '
                    'was renamed to "%s" in Beautiful Soup 4.0.0'
                    % (old_name, new_name),
                    DeprecationWarning,
                    stacklevel=3,
                )
                return kwargs.pop(old_name)
            return None

        parse_only = parse_only or deprecated_argument("parseOnlyThese", "parse_only")
        if parse_only is not None:
            # Issue a warning if we can tell in advance that
            # parse_only will exclude the entire tree.
            if parse_only.excludes_everything:
                warnings.warn(
                    f"The given value for parse_only will exclude everything: {parse_only}",
                    UserWarning,
                    stacklevel=3,
                )

        from_encoding = from_encoding or deprecated_argument(
            "fromEncoding", "from_encoding"
        )

        if from_encoding and isinstance(markup, str):
            warnings.warn(
                "You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored."
            )
            from_encoding = None

        self.element_classes = element_classes or dict()

        # We need this information to track whether or not the builder
        # was specified well enough that we can omit the 'you need to
        # specify a parser' warning.
        original_builder = builder
        original_features = features

        builder_class: Optional[Type[TreeBuilder]] = None
        if isinstance(builder, type):
            # A builder class was passed in; it needs to be instantiated.
            builder_class = builder
            builder = None
        elif builder is None:
            if isinstance(features, str):
                features = [features]
            if features is None or len(features) == 0:
                features = self.DEFAULT_BUILDER_FEATURES
            possible_builder_class = builder_registry.lookup(*features)
            if possible_builder_class is None:
>               raise FeatureNotFound(
                    "Couldn't find a tree builder with the features you "
                    "requested: %s. Do you need to install a parser library?"
                    % ",".join(features)
                )
E               bs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?

/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/bs4/__init__.py:366: FeatureNotFound
_________________ test_strategy_contextual_search_query_match __________________

    def test_strategy_contextual_search_query_match():
        html = '<a href="/details.php?id=456">My Show S01E01 1080p</a>'
>       soup = BeautifulSoup(html, "lxml")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/services/test_scraping_service.py:941:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <[AttributeError("'<class 'bs4.BeautifulSoup'>' object has no attribute 'contents'") raised in repr()] BeautifulSoup object at 0x7ff9863b8a10>
markup = '<a href="/details.php?id=456">My Show S01E01 1080p</a>'
features = ['lxml'], builder = None, parse_only = None, from_encoding = None
exclude_encodings = None, element_classes = None, kwargs = {}
deprecated_argument = <function BeautifulSoup.__init__.<locals>.deprecated_argument at 0x7ff9862b0400>
original_builder = None, original_features = 'lxml', builder_class = None
possible_builder_class = None

    def __init__(
        self,
        markup: _IncomingMarkup = "",
        features: Optional[Union[str, Sequence[str]]] = None,
        builder: Optional[Union[TreeBuilder, Type[TreeBuilder]]] = None,
        parse_only: Optional[SoupStrainer] = None,
        from_encoding: Optional[_Encoding] = None,
        exclude_encodings: Optional[_Encodings] = None,
        element_classes: Optional[Dict[Type[PageElement], Type[PageElement]]] = None,
        **kwargs: Any,
    ):
        """Constructor.

        :param markup: A string or a file-like object representing
         markup to be parsed.

        :param features: Desirable features of the parser to be
         used. This may be the name of a specific parser ("lxml",
         "lxml-xml", "html.parser", or "html5lib") or it may be the
         type of markup to be used ("html", "html5", "xml"). It's
         recommended that you name a specific parser, so that
         Beautiful Soup gives you the same results across platforms
         and virtual environments.

        :param builder: A TreeBuilder subclass to instantiate (or
         instance to use) instead of looking one up based on
         `features`. You only need to use this if you've implemented a
         custom TreeBuilder.

        :param parse_only: A SoupStrainer. Only parts of the document
         matching the SoupStrainer will be considered. This is useful
         when parsing part of a document that would otherwise be too
         large to fit into memory.

        :param from_encoding: A string indicating the encoding of the
         document to be parsed. Pass this in if Beautiful Soup is
         guessing wrongly about the document's encoding.

        :param exclude_encodings: A list of strings indicating
         encodings known to be wrong. Pass this in if you don't know
         the document's encoding but you know Beautiful Soup's guess is
         wrong.

        :param element_classes: A dictionary mapping BeautifulSoup
         classes like Tag and NavigableString, to other classes you'd
         like to be instantiated instead as the parse tree is
         built. This is useful for subclassing Tag or NavigableString
         to modify default behavior.

        :param kwargs: For backwards compatibility purposes, the
         constructor accepts certain keyword arguments used in
         Beautiful Soup 3. None of these arguments do anything in
         Beautiful Soup 4; they will result in a warning and then be
         ignored.

         Apart from this, any keyword arguments passed into the
         BeautifulSoup constructor are propagated to the TreeBuilder
         constructor. This makes it possible to configure a
         TreeBuilder by passing in arguments, not just by saying which
         one to use.
        """
        if "convertEntities" in kwargs:
            del kwargs["convertEntities"]
            warnings.warn(
                "BS4 does not respect the convertEntities argument to the "
                "BeautifulSoup constructor. Entities are always converted "
                "to Unicode characters."
            )

        if "markupMassage" in kwargs:
            del kwargs["markupMassage"]
            warnings.warn(
                "BS4 does not respect the markupMassage argument to the "
                "BeautifulSoup constructor. The tree builder is responsible "
                "for any necessary markup massage."
            )

        if "smartQuotesTo" in kwargs:
            del kwargs["smartQuotesTo"]
            warnings.warn(
                "BS4 does not respect the smartQuotesTo argument to the "
                "BeautifulSoup constructor. Smart quotes are always converted "
                "to Unicode characters."
            )

        if "selfClosingTags" in kwargs:
            del kwargs["selfClosingTags"]
            warnings.warn(
                "Beautiful Soup 4 does not respect the selfClosingTags argument to the "
                "BeautifulSoup constructor. The tree builder is responsible "
                "for understanding self-closing tags."
            )

        if "isHTML" in kwargs:
            del kwargs["isHTML"]
            warnings.warn(
                "Beautiful Soup 4 does not respect the isHTML argument to the "
                "BeautifulSoup constructor. Suggest you use "
                "features='lxml' for HTML and features='lxml-xml' for "
                "XML."
            )

        def deprecated_argument(old_name: str, new_name: str) -> Optional[Any]:
            if old_name in kwargs:
                warnings.warn(
                    'The "%s" argument to the BeautifulSoup constructor '
                    'was renamed to "%s" in Beautiful Soup 4.0.0'
                    % (old_name, new_name),
                    DeprecationWarning,
                    stacklevel=3,
                )
                return kwargs.pop(old_name)
            return None

        parse_only = parse_only or deprecated_argument("parseOnlyThese", "parse_only")
        if parse_only is not None:
            # Issue a warning if we can tell in advance that
            # parse_only will exclude the entire tree.
            if parse_only.excludes_everything:
                warnings.warn(
                    f"The given value for parse_only will exclude everything: {parse_only}",
                    UserWarning,
                    stacklevel=3,
                )

        from_encoding = from_encoding or deprecated_argument(
            "fromEncoding", "from_encoding"
        )

        if from_encoding and isinstance(markup, str):
            warnings.warn(
                "You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored."
            )
            from_encoding = None

        self.element_classes = element_classes or dict()

        # We need this information to track whether or not the builder
        # was specified well enough that we can omit the 'you need to
        # specify a parser' warning.
        original_builder = builder
        original_features = features

        builder_class: Optional[Type[TreeBuilder]] = None
        if isinstance(builder, type):
            # A builder class was passed in; it needs to be instantiated.
            builder_class = builder
            builder = None
        elif builder is None:
            if isinstance(features, str):
                features = [features]
            if features is None or len(features) == 0:
                features = self.DEFAULT_BUILDER_FEATURES
            possible_builder_class = builder_registry.lookup(*features)
            if possible_builder_class is None:
>               raise FeatureNotFound(
                    "Couldn't find a tree builder with the features you "
                    "requested: %s. Do you need to install a parser library?"
                    % ",".join(features)
                )
E               bs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?

/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/bs4/__init__.py:366: FeatureNotFound
______________ test_strategy_contextual_search_unrelated_keyword _______________

    def test_strategy_contextual_search_unrelated_keyword():
        html = '<a href="/about">About our download policy</a>'
>       soup = BeautifulSoup(html, "lxml")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/services/test_scraping_service.py:948:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <[AttributeError("'<class 'bs4.BeautifulSoup'>' object has no attribute 'contents'") raised in repr()] BeautifulSoup object at 0x7ff9863bb470>
markup = '<a href="/about">About our download policy</a>', features = ['lxml']
builder = None, parse_only = None, from_encoding = None
exclude_encodings = None, element_classes = None, kwargs = {}
deprecated_argument = <function BeautifulSoup.__init__.<locals>.deprecated_argument at 0x7ff9862b3b00>
original_builder = None, original_features = 'lxml', builder_class = None
possible_builder_class = None

    def __init__(
        self,
        markup: _IncomingMarkup = "",
        features: Optional[Union[str, Sequence[str]]] = None,
        builder: Optional[Union[TreeBuilder, Type[TreeBuilder]]] = None,
        parse_only: Optional[SoupStrainer] = None,
        from_encoding: Optional[_Encoding] = None,
        exclude_encodings: Optional[_Encodings] = None,
        element_classes: Optional[Dict[Type[PageElement], Type[PageElement]]] = None,
        **kwargs: Any,
    ):
        """Constructor.

        :param markup: A string or a file-like object representing
         markup to be parsed.

        :param features: Desirable features of the parser to be
         used. This may be the name of a specific parser ("lxml",
         "lxml-xml", "html.parser", or "html5lib") or it may be the
         type of markup to be used ("html", "html5", "xml"). It's
         recommended that you name a specific parser, so that
         Beautiful Soup gives you the same results across platforms
         and virtual environments.

        :param builder: A TreeBuilder subclass to instantiate (or
         instance to use) instead of looking one up based on
         `features`. You only need to use this if you've implemented a
         custom TreeBuilder.

        :param parse_only: A SoupStrainer. Only parts of the document
         matching the SoupStrainer will be considered. This is useful
         when parsing part of a document that would otherwise be too
         large to fit into memory.

        :param from_encoding: A string indicating the encoding of the
         document to be parsed. Pass this in if Beautiful Soup is
         guessing wrongly about the document's encoding.

        :param exclude_encodings: A list of strings indicating
         encodings known to be wrong. Pass this in if you don't know
         the document's encoding but you know Beautiful Soup's guess is
         wrong.

        :param element_classes: A dictionary mapping BeautifulSoup
         classes like Tag and NavigableString, to other classes you'd
         like to be instantiated instead as the parse tree is
         built. This is useful for subclassing Tag or NavigableString
         to modify default behavior.

        :param kwargs: For backwards compatibility purposes, the
         constructor accepts certain keyword arguments used in
         Beautiful Soup 3. None of these arguments do anything in
         Beautiful Soup 4; they will result in a warning and then be
         ignored.

         Apart from this, any keyword arguments passed into the
         BeautifulSoup constructor are propagated to the TreeBuilder
         constructor. This makes it possible to configure a
         TreeBuilder by passing in arguments, not just by saying which
         one to use.
        """
        if "convertEntities" in kwargs:
            del kwargs["convertEntities"]
            warnings.warn(
                "BS4 does not respect the convertEntities argument to the "
                "BeautifulSoup constructor. Entities are always converted "
                "to Unicode characters."
            )

        if "markupMassage" in kwargs:
            del kwargs["markupMassage"]
            warnings.warn(
                "BS4 does not respect the markupMassage argument to the "
                "BeautifulSoup constructor. The tree builder is responsible "
                "for any necessary markup massage."
            )

        if "smartQuotesTo" in kwargs:
            del kwargs["smartQuotesTo"]
            warnings.warn(
                "BS4 does not respect the smartQuotesTo argument to the "
                "BeautifulSoup constructor. Smart quotes are always converted "
                "to Unicode characters."
            )

        if "selfClosingTags" in kwargs:
            del kwargs["selfClosingTags"]
            warnings.warn(
                "Beautiful Soup 4 does not respect the selfClosingTags argument to the "
                "BeautifulSoup constructor. The tree builder is responsible "
                "for understanding self-closing tags."
            )

        if "isHTML" in kwargs:
            del kwargs["isHTML"]
            warnings.warn(
                "Beautiful Soup 4 does not respect the isHTML argument to the "
                "BeautifulSoup constructor. Suggest you use "
                "features='lxml' for HTML and features='lxml-xml' for "
                "XML."
            )

        def deprecated_argument(old_name: str, new_name: str) -> Optional[Any]:
            if old_name in kwargs:
                warnings.warn(
                    'The "%s" argument to the BeautifulSoup constructor '
                    'was renamed to "%s" in Beautiful Soup 4.0.0'
                    % (old_name, new_name),
                    DeprecationWarning,
                    stacklevel=3,
                )
                return kwargs.pop(old_name)
            return None

        parse_only = parse_only or deprecated_argument("parseOnlyThese", "parse_only")
        if parse_only is not None:
            # Issue a warning if we can tell in advance that
            # parse_only will exclude the entire tree.
            if parse_only.excludes_everything:
                warnings.warn(
                    f"The given value for parse_only will exclude everything: {parse_only}",
                    UserWarning,
                    stacklevel=3,
                )

        from_encoding = from_encoding or deprecated_argument(
            "fromEncoding", "from_encoding"
        )

        if from_encoding and isinstance(markup, str):
            warnings.warn(
                "You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored."
            )
            from_encoding = None

        self.element_classes = element_classes or dict()

        # We need this information to track whether or not the builder
        # was specified well enough that we can omit the 'you need to
        # specify a parser' warning.
        original_builder = builder
        original_features = features

        builder_class: Optional[Type[TreeBuilder]] = None
        if isinstance(builder, type):
            # A builder class was passed in; it needs to be instantiated.
            builder_class = builder
            builder = None
        elif builder is None:
            if isinstance(features, str):
                features = [features]
            if features is None or len(features) == 0:
                features = self.DEFAULT_BUILDER_FEATURES
            possible_builder_class = builder_registry.lookup(*features)
            if possible_builder_class is None:
>               raise FeatureNotFound(
                    "Couldn't find a tree builder with the features you "
                    "requested: %s. Do you need to install a parser library?"
                    % ",".join(features)
                )
E               bs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?

/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/bs4/__init__.py:366: FeatureNotFound
__________________ test_strategy_find_in_tables_single_match ___________________

    def test_strategy_find_in_tables_single_match():
        html = '<table><tr><td>My Show</td><td><a href="/dl">Download</a></td></tr></table>'
>       soup = BeautifulSoup(html, "lxml")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/services/test_scraping_service.py:955:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <[AttributeError("'<class 'bs4.BeautifulSoup'>' object has no attribute 'contents'") raised in repr()] BeautifulSoup object at 0x7ff9863bb3b0>
markup = '<table><tr><td>My Show</td><td><a href="/dl">Download</a></td></tr></table>'
features = ['lxml'], builder = None, parse_only = None, from_encoding = None
exclude_encodings = None, element_classes = None, kwargs = {}
deprecated_argument = <function BeautifulSoup.__init__.<locals>.deprecated_argument at 0x7ff9862b2ac0>
original_builder = None, original_features = 'lxml', builder_class = None
possible_builder_class = None

    def __init__(
        self,
        markup: _IncomingMarkup = "",
        features: Optional[Union[str, Sequence[str]]] = None,
        builder: Optional[Union[TreeBuilder, Type[TreeBuilder]]] = None,
        parse_only: Optional[SoupStrainer] = None,
        from_encoding: Optional[_Encoding] = None,
        exclude_encodings: Optional[_Encodings] = None,
        element_classes: Optional[Dict[Type[PageElement], Type[PageElement]]] = None,
        **kwargs: Any,
    ):
        """Constructor.

        :param markup: A string or a file-like object representing
         markup to be parsed.

        :param features: Desirable features of the parser to be
         used. This may be the name of a specific parser ("lxml",
         "lxml-xml", "html.parser", or "html5lib") or it may be the
         type of markup to be used ("html", "html5", "xml"). It's
         recommended that you name a specific parser, so that
         Beautiful Soup gives you the same results across platforms
         and virtual environments.

        :param builder: A TreeBuilder subclass to instantiate (or
         instance to use) instead of looking one up based on
         `features`. You only need to use this if you've implemented a
         custom TreeBuilder.

        :param parse_only: A SoupStrainer. Only parts of the document
         matching the SoupStrainer will be considered. This is useful
         when parsing part of a document that would otherwise be too
         large to fit into memory.

        :param from_encoding: A string indicating the encoding of the
         document to be parsed. Pass this in if Beautiful Soup is
         guessing wrongly about the document's encoding.

        :param exclude_encodings: A list of strings indicating
         encodings known to be wrong. Pass this in if you don't know
         the document's encoding but you know Beautiful Soup's guess is
         wrong.

        :param element_classes: A dictionary mapping BeautifulSoup
         classes like Tag and NavigableString, to other classes you'd
         like to be instantiated instead as the parse tree is
         built. This is useful for subclassing Tag or NavigableString
         to modify default behavior.

        :param kwargs: For backwards compatibility purposes, the
         constructor accepts certain keyword arguments used in
         Beautiful Soup 3. None of these arguments do anything in
         Beautiful Soup 4; they will result in a warning and then be
         ignored.

         Apart from this, any keyword arguments passed into the
         BeautifulSoup constructor are propagated to the TreeBuilder
         constructor. This makes it possible to configure a
         TreeBuilder by passing in arguments, not just by saying which
         one to use.
        """
        if "convertEntities" in kwargs:
            del kwargs["convertEntities"]
            warnings.warn(
                "BS4 does not respect the convertEntities argument to the "
                "BeautifulSoup constructor. Entities are always converted "
                "to Unicode characters."
            )

        if "markupMassage" in kwargs:
            del kwargs["markupMassage"]
            warnings.warn(
                "BS4 does not respect the markupMassage argument to the "
                "BeautifulSoup constructor. The tree builder is responsible "
                "for any necessary markup massage."
            )

        if "smartQuotesTo" in kwargs:
            del kwargs["smartQuotesTo"]
            warnings.warn(
                "BS4 does not respect the smartQuotesTo argument to the "
                "BeautifulSoup constructor. Smart quotes are always converted "
                "to Unicode characters."
            )

        if "selfClosingTags" in kwargs:
            del kwargs["selfClosingTags"]
            warnings.warn(
                "Beautiful Soup 4 does not respect the selfClosingTags argument to the "
                "BeautifulSoup constructor. The tree builder is responsible "
                "for understanding self-closing tags."
            )

        if "isHTML" in kwargs:
            del kwargs["isHTML"]
            warnings.warn(
                "Beautiful Soup 4 does not respect the isHTML argument to the "
                "BeautifulSoup constructor. Suggest you use "
                "features='lxml' for HTML and features='lxml-xml' for "
                "XML."
            )

        def deprecated_argument(old_name: str, new_name: str) -> Optional[Any]:
            if old_name in kwargs:
                warnings.warn(
                    'The "%s" argument to the BeautifulSoup constructor '
                    'was renamed to "%s" in Beautiful Soup 4.0.0'
                    % (old_name, new_name),
                    DeprecationWarning,
                    stacklevel=3,
                )
                return kwargs.pop(old_name)
            return None

        parse_only = parse_only or deprecated_argument("parseOnlyThese", "parse_only")
        if parse_only is not None:
            # Issue a warning if we can tell in advance that
            # parse_only will exclude the entire tree.
            if parse_only.excludes_everything:
                warnings.warn(
                    f"The given value for parse_only will exclude everything: {parse_only}",
                    UserWarning,
                    stacklevel=3,
                )

        from_encoding = from_encoding or deprecated_argument(
            "fromEncoding", "from_encoding"
        )

        if from_encoding and isinstance(markup, str):
            warnings.warn(
                "You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored."
            )
            from_encoding = None

        self.element_classes = element_classes or dict()

        # We need this information to track whether or not the builder
        # was specified well enough that we can omit the 'you need to
        # specify a parser' warning.
        original_builder = builder
        original_features = features

        builder_class: Optional[Type[TreeBuilder]] = None
        if isinstance(builder, type):
            # A builder class was passed in; it needs to be instantiated.
            builder_class = builder
            builder = None
        elif builder is None:
            if isinstance(features, str):
                features = [features]
            if features is None or len(features) == 0:
                features = self.DEFAULT_BUILDER_FEATURES
            possible_builder_class = builder_registry.lookup(*features)
            if possible_builder_class is None:
>               raise FeatureNotFound(
                    "Couldn't find a tree builder with the features you "
                    "requested: %s. Do you need to install a parser library?"
                    % ",".join(features)
                )
E               bs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?

/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/bs4/__init__.py:366: FeatureNotFound
________________ test_strategy_find_in_tables_multiple_matches _________________

    def test_strategy_find_in_tables_multiple_matches():
        html = """
        <table>
          <tr><td>My Show S01E01</td><td><a href="/e1">DL</a></td></tr>
          <tr><td>My Show S01E02</td><td><a href="/e2">DL</a></td></tr>
        </table>
        """
>       soup = BeautifulSoup(html, "lxml")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/services/test_scraping_service.py:967:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <[AttributeError("'<class 'bs4.BeautifulSoup'>' object has no attribute 'contents'") raised in repr()] BeautifulSoup object at 0x7ff9863b9f10>
markup = '\n    <table>\n      <tr><td>My Show S01E01</td><td><a href="/e1">DL</a></td></tr>\n      <tr><td>My Show S01E02</td><td><a href="/e2">DL</a></td></tr>\n    </table>\n    '
features = ['lxml'], builder = None, parse_only = None, from_encoding = None
exclude_encodings = None, element_classes = None, kwargs = {}
deprecated_argument = <function BeautifulSoup.__init__.<locals>.deprecated_argument at 0x7ff9862b0720>
original_builder = None, original_features = 'lxml', builder_class = None
possible_builder_class = None

    def __init__(
        self,
        markup: _IncomingMarkup = "",
        features: Optional[Union[str, Sequence[str]]] = None,
        builder: Optional[Union[TreeBuilder, Type[TreeBuilder]]] = None,
        parse_only: Optional[SoupStrainer] = None,
        from_encoding: Optional[_Encoding] = None,
        exclude_encodings: Optional[_Encodings] = None,
        element_classes: Optional[Dict[Type[PageElement], Type[PageElement]]] = None,
        **kwargs: Any,
    ):
        """Constructor.

        :param markup: A string or a file-like object representing
         markup to be parsed.

        :param features: Desirable features of the parser to be
         used. This may be the name of a specific parser ("lxml",
         "lxml-xml", "html.parser", or "html5lib") or it may be the
         type of markup to be used ("html", "html5", "xml"). It's
         recommended that you name a specific parser, so that
         Beautiful Soup gives you the same results across platforms
         and virtual environments.

        :param builder: A TreeBuilder subclass to instantiate (or
         instance to use) instead of looking one up based on
         `features`. You only need to use this if you've implemented a
         custom TreeBuilder.

        :param parse_only: A SoupStrainer. Only parts of the document
         matching the SoupStrainer will be considered. This is useful
         when parsing part of a document that would otherwise be too
         large to fit into memory.

        :param from_encoding: A string indicating the encoding of the
         document to be parsed. Pass this in if Beautiful Soup is
         guessing wrongly about the document's encoding.

        :param exclude_encodings: A list of strings indicating
         encodings known to be wrong. Pass this in if you don't know
         the document's encoding but you know Beautiful Soup's guess is
         wrong.

        :param element_classes: A dictionary mapping BeautifulSoup
         classes like Tag and NavigableString, to other classes you'd
         like to be instantiated instead as the parse tree is
         built. This is useful for subclassing Tag or NavigableString
         to modify default behavior.

        :param kwargs: For backwards compatibility purposes, the
         constructor accepts certain keyword arguments used in
         Beautiful Soup 3. None of these arguments do anything in
         Beautiful Soup 4; they will result in a warning and then be
         ignored.

         Apart from this, any keyword arguments passed into the
         BeautifulSoup constructor are propagated to the TreeBuilder
         constructor. This makes it possible to configure a
         TreeBuilder by passing in arguments, not just by saying which
         one to use.
        """
        if "convertEntities" in kwargs:
            del kwargs["convertEntities"]
            warnings.warn(
                "BS4 does not respect the convertEntities argument to the "
                "BeautifulSoup constructor. Entities are always converted "
                "to Unicode characters."
            )

        if "markupMassage" in kwargs:
            del kwargs["markupMassage"]
            warnings.warn(
                "BS4 does not respect the markupMassage argument to the "
                "BeautifulSoup constructor. The tree builder is responsible "
                "for any necessary markup massage."
            )

        if "smartQuotesTo" in kwargs:
            del kwargs["smartQuotesTo"]
            warnings.warn(
                "BS4 does not respect the smartQuotesTo argument to the "
                "BeautifulSoup constructor. Smart quotes are always converted "
                "to Unicode characters."
            )

        if "selfClosingTags" in kwargs:
            del kwargs["selfClosingTags"]
            warnings.warn(
                "Beautiful Soup 4 does not respect the selfClosingTags argument to the "
                "BeautifulSoup constructor. The tree builder is responsible "
                "for understanding self-closing tags."
            )

        if "isHTML" in kwargs:
            del kwargs["isHTML"]
            warnings.warn(
                "Beautiful Soup 4 does not respect the isHTML argument to the "
                "BeautifulSoup constructor. Suggest you use "
                "features='lxml' for HTML and features='lxml-xml' for "
                "XML."
            )

        def deprecated_argument(old_name: str, new_name: str) -> Optional[Any]:
            if old_name in kwargs:
                warnings.warn(
                    'The "%s" argument to the BeautifulSoup constructor '
                    'was renamed to "%s" in Beautiful Soup 4.0.0'
                    % (old_name, new_name),
                    DeprecationWarning,
                    stacklevel=3,
                )
                return kwargs.pop(old_name)
            return None

        parse_only = parse_only or deprecated_argument("parseOnlyThese", "parse_only")
        if parse_only is not None:
            # Issue a warning if we can tell in advance that
            # parse_only will exclude the entire tree.
            if parse_only.excludes_everything:
                warnings.warn(
                    f"The given value for parse_only will exclude everything: {parse_only}",
                    UserWarning,
                    stacklevel=3,
                )

        from_encoding = from_encoding or deprecated_argument(
            "fromEncoding", "from_encoding"
        )

        if from_encoding and isinstance(markup, str):
            warnings.warn(
                "You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored."
            )
            from_encoding = None

        self.element_classes = element_classes or dict()

        # We need this information to track whether or not the builder
        # was specified well enough that we can omit the 'you need to
        # specify a parser' warning.
        original_builder = builder
        original_features = features

        builder_class: Optional[Type[TreeBuilder]] = None
        if isinstance(builder, type):
            # A builder class was passed in; it needs to be instantiated.
            builder_class = builder
            builder = None
        elif builder is None:
            if isinstance(features, str):
                features = [features]
            if features is None or len(features) == 0:
                features = self.DEFAULT_BUILDER_FEATURES
            possible_builder_class = builder_registry.lookup(*features)
            if possible_builder_class is None:
>               raise FeatureNotFound(
                    "Couldn't find a tree builder with the features you "
                    "requested: %s. Do you need to install a parser library?"
                    % ",".join(features)
                )
E               bs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?

/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/bs4/__init__.py:366: FeatureNotFound
____________ test_strategy_find_in_tables_ignores_unrelated_tables _____________

    def test_strategy_find_in_tables_ignores_unrelated_tables():
        html = """
        <table><tr><td>Other</td><td><a href="/x">X</a></td></tr></table>
        <table><tr><td>My Show</td><td><a href="/dl">Download</a></td></tr></table>
        """
>       soup = BeautifulSoup(html, "lxml")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/services/test_scraping_service.py:977:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <[AttributeError("'<class 'bs4.BeautifulSoup'>' object has no attribute 'contents'") raised in repr()] BeautifulSoup object at 0x7ff986402ea0>
markup = '\n    <table><tr><td>Other</td><td><a href="/x">X</a></td></tr></table>\n    <table><tr><td>My Show</td><td><a href="/dl">Download</a></td></tr></table>\n    '
features = ['lxml'], builder = None, parse_only = None, from_encoding = None
exclude_encodings = None, element_classes = None, kwargs = {}
deprecated_argument = <function BeautifulSoup.__init__.<locals>.deprecated_argument at 0x7ff9863ec4a0>
original_builder = None, original_features = 'lxml', builder_class = None
possible_builder_class = None

    def __init__(
        self,
        markup: _IncomingMarkup = "",
        features: Optional[Union[str, Sequence[str]]] = None,
        builder: Optional[Union[TreeBuilder, Type[TreeBuilder]]] = None,
        parse_only: Optional[SoupStrainer] = None,
        from_encoding: Optional[_Encoding] = None,
        exclude_encodings: Optional[_Encodings] = None,
        element_classes: Optional[Dict[Type[PageElement], Type[PageElement]]] = None,
        **kwargs: Any,
    ):
        """Constructor.

        :param markup: A string or a file-like object representing
         markup to be parsed.

        :param features: Desirable features of the parser to be
         used. This may be the name of a specific parser ("lxml",
         "lxml-xml", "html.parser", or "html5lib") or it may be the
         type of markup to be used ("html", "html5", "xml"). It's
         recommended that you name a specific parser, so that
         Beautiful Soup gives you the same results across platforms
         and virtual environments.

        :param builder: A TreeBuilder subclass to instantiate (or
         instance to use) instead of looking one up based on
         `features`. You only need to use this if you've implemented a
         custom TreeBuilder.

        :param parse_only: A SoupStrainer. Only parts of the document
         matching the SoupStrainer will be considered. This is useful
         when parsing part of a document that would otherwise be too
         large to fit into memory.

        :param from_encoding: A string indicating the encoding of the
         document to be parsed. Pass this in if Beautiful Soup is
         guessing wrongly about the document's encoding.

        :param exclude_encodings: A list of strings indicating
         encodings known to be wrong. Pass this in if you don't know
         the document's encoding but you know Beautiful Soup's guess is
         wrong.

        :param element_classes: A dictionary mapping BeautifulSoup
         classes like Tag and NavigableString, to other classes you'd
         like to be instantiated instead as the parse tree is
         built. This is useful for subclassing Tag or NavigableString
         to modify default behavior.

        :param kwargs: For backwards compatibility purposes, the
         constructor accepts certain keyword arguments used in
         Beautiful Soup 3. None of these arguments do anything in
         Beautiful Soup 4; they will result in a warning and then be
         ignored.

         Apart from this, any keyword arguments passed into the
         BeautifulSoup constructor are propagated to the TreeBuilder
         constructor. This makes it possible to configure a
         TreeBuilder by passing in arguments, not just by saying which
         one to use.
        """
        if "convertEntities" in kwargs:
            del kwargs["convertEntities"]
            warnings.warn(
                "BS4 does not respect the convertEntities argument to the "
                "BeautifulSoup constructor. Entities are always converted "
                "to Unicode characters."
            )

        if "markupMassage" in kwargs:
            del kwargs["markupMassage"]
            warnings.warn(
                "BS4 does not respect the markupMassage argument to the "
                "BeautifulSoup constructor. The tree builder is responsible "
                "for any necessary markup massage."
            )

        if "smartQuotesTo" in kwargs:
            del kwargs["smartQuotesTo"]
            warnings.warn(
                "BS4 does not respect the smartQuotesTo argument to the "
                "BeautifulSoup constructor. Smart quotes are always converted "
                "to Unicode characters."
            )

        if "selfClosingTags" in kwargs:
            del kwargs["selfClosingTags"]
            warnings.warn(
                "Beautiful Soup 4 does not respect the selfClosingTags argument to the "
                "BeautifulSoup constructor. The tree builder is responsible "
                "for understanding self-closing tags."
            )

        if "isHTML" in kwargs:
            del kwargs["isHTML"]
            warnings.warn(
                "Beautiful Soup 4 does not respect the isHTML argument to the "
                "BeautifulSoup constructor. Suggest you use "
                "features='lxml' for HTML and features='lxml-xml' for "
                "XML."
            )

        def deprecated_argument(old_name: str, new_name: str) -> Optional[Any]:
            if old_name in kwargs:
                warnings.warn(
                    'The "%s" argument to the BeautifulSoup constructor '
                    'was renamed to "%s" in Beautiful Soup 4.0.0'
                    % (old_name, new_name),
                    DeprecationWarning,
                    stacklevel=3,
                )
                return kwargs.pop(old_name)
            return None

        parse_only = parse_only or deprecated_argument("parseOnlyThese", "parse_only")
        if parse_only is not None:
            # Issue a warning if we can tell in advance that
            # parse_only will exclude the entire tree.
            if parse_only.excludes_everything:
                warnings.warn(
                    f"The given value for parse_only will exclude everything: {parse_only}",
                    UserWarning,
                    stacklevel=3,
                )

        from_encoding = from_encoding or deprecated_argument(
            "fromEncoding", "from_encoding"
        )

        if from_encoding and isinstance(markup, str):
            warnings.warn(
                "You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored."
            )
            from_encoding = None

        self.element_classes = element_classes or dict()

        # We need this information to track whether or not the builder
        # was specified well enough that we can omit the 'you need to
        # specify a parser' warning.
        original_builder = builder
        original_features = features

        builder_class: Optional[Type[TreeBuilder]] = None
        if isinstance(builder, type):
            # A builder class was passed in; it needs to be instantiated.
            builder_class = builder
            builder = None
        elif builder is None:
            if isinstance(features, str):
                features = [features]
            if features is None or len(features) == 0:
                features = self.DEFAULT_BUILDER_FEATURES
            possible_builder_class = builder_registry.lookup(*features)
            if possible_builder_class is None:
>               raise FeatureNotFound(
                    "Couldn't find a tree builder with the features you "
                    "requested: %s. Do you need to install a parser library?"
                    % ",".join(features)
                )
E               bs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?

/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/bs4/__init__.py:366: FeatureNotFound
__________________ test_score_candidate_links_prefers_magnet ___________________

    def test_score_candidate_links_prefers_magnet():
        html = (
            '<div><a href="magnet:?xt=urn:btih:1">Magnet</a></div>'
            '<div><a href="/context">Download Torrent</a></div>'
            '<table><tr><td>My Show</td><td><a href="/table">Link</a></td></tr></table>'
        )
>       soup = BeautifulSoup(html, "lxml")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/services/test_scraping_service.py:988:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <[AttributeError("'<class 'bs4.BeautifulSoup'>' object has no attribute 'contents'") raised in repr()] BeautifulSoup object at 0x7ff9863ba7e0>
markup = '<div><a href="magnet:?xt=urn:btih:1">Magnet</a></div><div><a href="/context">Download Torrent</a></div><table><tr><td>My Show</td><td><a href="/table">Link</a></td></tr></table>'
features = ['lxml'], builder = None, parse_only = None, from_encoding = None
exclude_encodings = None, element_classes = None, kwargs = {}
deprecated_argument = <function BeautifulSoup.__init__.<locals>.deprecated_argument at 0x7ff9862b3a60>
original_builder = None, original_features = 'lxml', builder_class = None
possible_builder_class = None

    def __init__(
        self,
        markup: _IncomingMarkup = "",
        features: Optional[Union[str, Sequence[str]]] = None,
        builder: Optional[Union[TreeBuilder, Type[TreeBuilder]]] = None,
        parse_only: Optional[SoupStrainer] = None,
        from_encoding: Optional[_Encoding] = None,
        exclude_encodings: Optional[_Encodings] = None,
        element_classes: Optional[Dict[Type[PageElement], Type[PageElement]]] = None,
        **kwargs: Any,
    ):
        """Constructor.

        :param markup: A string or a file-like object representing
         markup to be parsed.

        :param features: Desirable features of the parser to be
         used. This may be the name of a specific parser ("lxml",
         "lxml-xml", "html.parser", or "html5lib") or it may be the
         type of markup to be used ("html", "html5", "xml"). It's
         recommended that you name a specific parser, so that
         Beautiful Soup gives you the same results across platforms
         and virtual environments.

        :param builder: A TreeBuilder subclass to instantiate (or
         instance to use) instead of looking one up based on
         `features`. You only need to use this if you've implemented a
         custom TreeBuilder.

        :param parse_only: A SoupStrainer. Only parts of the document
         matching the SoupStrainer will be considered. This is useful
         when parsing part of a document that would otherwise be too
         large to fit into memory.

        :param from_encoding: A string indicating the encoding of the
         document to be parsed. Pass this in if Beautiful Soup is
         guessing wrongly about the document's encoding.

        :param exclude_encodings: A list of strings indicating
         encodings known to be wrong. Pass this in if you don't know
         the document's encoding but you know Beautiful Soup's guess is
         wrong.

        :param element_classes: A dictionary mapping BeautifulSoup
         classes like Tag and NavigableString, to other classes you'd
         like to be instantiated instead as the parse tree is
         built. This is useful for subclassing Tag or NavigableString
         to modify default behavior.

        :param kwargs: For backwards compatibility purposes, the
         constructor accepts certain keyword arguments used in
         Beautiful Soup 3. None of these arguments do anything in
         Beautiful Soup 4; they will result in a warning and then be
         ignored.

         Apart from this, any keyword arguments passed into the
         BeautifulSoup constructor are propagated to the TreeBuilder
         constructor. This makes it possible to configure a
         TreeBuilder by passing in arguments, not just by saying which
         one to use.
        """
        if "convertEntities" in kwargs:
            del kwargs["convertEntities"]
            warnings.warn(
                "BS4 does not respect the convertEntities argument to the "
                "BeautifulSoup constructor. Entities are always converted "
                "to Unicode characters."
            )

        if "markupMassage" in kwargs:
            del kwargs["markupMassage"]
            warnings.warn(
                "BS4 does not respect the markupMassage argument to the "
                "BeautifulSoup constructor. The tree builder is responsible "
                "for any necessary markup massage."
            )

        if "smartQuotesTo" in kwargs:
            del kwargs["smartQuotesTo"]
            warnings.warn(
                "BS4 does not respect the smartQuotesTo argument to the "
                "BeautifulSoup constructor. Smart quotes are always converted "
                "to Unicode characters."
            )

        if "selfClosingTags" in kwargs:
            del kwargs["selfClosingTags"]
            warnings.warn(
                "Beautiful Soup 4 does not respect the selfClosingTags argument to the "
                "BeautifulSoup constructor. The tree builder is responsible "
                "for understanding self-closing tags."
            )

        if "isHTML" in kwargs:
            del kwargs["isHTML"]
            warnings.warn(
                "Beautiful Soup 4 does not respect the isHTML argument to the "
                "BeautifulSoup constructor. Suggest you use "
                "features='lxml' for HTML and features='lxml-xml' for "
                "XML."
            )

        def deprecated_argument(old_name: str, new_name: str) -> Optional[Any]:
            if old_name in kwargs:
                warnings.warn(
                    'The "%s" argument to the BeautifulSoup constructor '
                    'was renamed to "%s" in Beautiful Soup 4.0.0'
                    % (old_name, new_name),
                    DeprecationWarning,
                    stacklevel=3,
                )
                return kwargs.pop(old_name)
            return None

        parse_only = parse_only or deprecated_argument("parseOnlyThese", "parse_only")
        if parse_only is not None:
            # Issue a warning if we can tell in advance that
            # parse_only will exclude the entire tree.
            if parse_only.excludes_everything:
                warnings.warn(
                    f"The given value for parse_only will exclude everything: {parse_only}",
                    UserWarning,
                    stacklevel=3,
                )

        from_encoding = from_encoding or deprecated_argument(
            "fromEncoding", "from_encoding"
        )

        if from_encoding and isinstance(markup, str):
            warnings.warn(
                "You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored."
            )
            from_encoding = None

        self.element_classes = element_classes or dict()

        # We need this information to track whether or not the builder
        # was specified well enough that we can omit the 'you need to
        # specify a parser' warning.
        original_builder = builder
        original_features = features

        builder_class: Optional[Type[TreeBuilder]] = None
        if isinstance(builder, type):
            # A builder class was passed in; it needs to be instantiated.
            builder_class = builder
            builder = None
        elif builder is None:
            if isinstance(features, str):
                features = [features]
            if features is None or len(features) == 0:
                features = self.DEFAULT_BUILDER_FEATURES
            possible_builder_class = builder_registry.lookup(*features)
            if possible_builder_class is None:
>               raise FeatureNotFound(
                    "Couldn't find a tree builder with the features you "
                    "requested: %s. Do you need to install a parser library?"
                    % ",".join(features)
                )
E               bs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?

/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/bs4/__init__.py:366: FeatureNotFound
___________________ test_score_candidate_links_penalizes_ads ___________________

    def test_score_candidate_links_penalizes_ads():
        html = (
            '<div class="ad"><a href="/bad">My Show 1080p</a></div>'
            '<div><a href="/good">My Show 1080p</a></div>'
        )
>       soup = BeautifulSoup(html, "lxml")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/services/test_scraping_service.py:1000:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <[AttributeError("'<class 'bs4.BeautifulSoup'>' object has no attribute 'contents'") raised in repr()] BeautifulSoup object at 0x7ff986401550>
markup = '<div class="ad"><a href="/bad">My Show 1080p</a></div><div><a href="/good">My Show 1080p</a></div>'
features = ['lxml'], builder = None, parse_only = None, from_encoding = None
exclude_encodings = None, element_classes = None, kwargs = {}
deprecated_argument = <function BeautifulSoup.__init__.<locals>.deprecated_argument at 0x7ff9862b3ec0>
original_builder = None, original_features = 'lxml', builder_class = None
possible_builder_class = None

    def __init__(
        self,
        markup: _IncomingMarkup = "",
        features: Optional[Union[str, Sequence[str]]] = None,
        builder: Optional[Union[TreeBuilder, Type[TreeBuilder]]] = None,
        parse_only: Optional[SoupStrainer] = None,
        from_encoding: Optional[_Encoding] = None,
        exclude_encodings: Optional[_Encodings] = None,
        element_classes: Optional[Dict[Type[PageElement], Type[PageElement]]] = None,
        **kwargs: Any,
    ):
        """Constructor.

        :param markup: A string or a file-like object representing
         markup to be parsed.

        :param features: Desirable features of the parser to be
         used. This may be the name of a specific parser ("lxml",
         "lxml-xml", "html.parser", or "html5lib") or it may be the
         type of markup to be used ("html", "html5", "xml"). It's
         recommended that you name a specific parser, so that
         Beautiful Soup gives you the same results across platforms
         and virtual environments.

        :param builder: A TreeBuilder subclass to instantiate (or
         instance to use) instead of looking one up based on
         `features`. You only need to use this if you've implemented a
         custom TreeBuilder.

        :param parse_only: A SoupStrainer. Only parts of the document
         matching the SoupStrainer will be considered. This is useful
         when parsing part of a document that would otherwise be too
         large to fit into memory.

        :param from_encoding: A string indicating the encoding of the
         document to be parsed. Pass this in if Beautiful Soup is
         guessing wrongly about the document's encoding.

        :param exclude_encodings: A list of strings indicating
         encodings known to be wrong. Pass this in if you don't know
         the document's encoding but you know Beautiful Soup's guess is
         wrong.

        :param element_classes: A dictionary mapping BeautifulSoup
         classes like Tag and NavigableString, to other classes you'd
         like to be instantiated instead as the parse tree is
         built. This is useful for subclassing Tag or NavigableString
         to modify default behavior.

        :param kwargs: For backwards compatibility purposes, the
         constructor accepts certain keyword arguments used in
         Beautiful Soup 3. None of these arguments do anything in
         Beautiful Soup 4; they will result in a warning and then be
         ignored.

         Apart from this, any keyword arguments passed into the
         BeautifulSoup constructor are propagated to the TreeBuilder
         constructor. This makes it possible to configure a
         TreeBuilder by passing in arguments, not just by saying which
         one to use.
        """
        if "convertEntities" in kwargs:
            del kwargs["convertEntities"]
            warnings.warn(
                "BS4 does not respect the convertEntities argument to the "
                "BeautifulSoup constructor. Entities are always converted "
                "to Unicode characters."
            )

        if "markupMassage" in kwargs:
            del kwargs["markupMassage"]
            warnings.warn(
                "BS4 does not respect the markupMassage argument to the "
                "BeautifulSoup constructor. The tree builder is responsible "
                "for any necessary markup massage."
            )

        if "smartQuotesTo" in kwargs:
            del kwargs["smartQuotesTo"]
            warnings.warn(
                "BS4 does not respect the smartQuotesTo argument to the "
                "BeautifulSoup constructor. Smart quotes are always converted "
                "to Unicode characters."
            )

        if "selfClosingTags" in kwargs:
            del kwargs["selfClosingTags"]
            warnings.warn(
                "Beautiful Soup 4 does not respect the selfClosingTags argument to the "
                "BeautifulSoup constructor. The tree builder is responsible "
                "for understanding self-closing tags."
            )

        if "isHTML" in kwargs:
            del kwargs["isHTML"]
            warnings.warn(
                "Beautiful Soup 4 does not respect the isHTML argument to the "
                "BeautifulSoup constructor. Suggest you use "
                "features='lxml' for HTML and features='lxml-xml' for "
                "XML."
            )

        def deprecated_argument(old_name: str, new_name: str) -> Optional[Any]:
            if old_name in kwargs:
                warnings.warn(
                    'The "%s" argument to the BeautifulSoup constructor '
                    'was renamed to "%s" in Beautiful Soup 4.0.0'
                    % (old_name, new_name),
                    DeprecationWarning,
                    stacklevel=3,
                )
                return kwargs.pop(old_name)
            return None

        parse_only = parse_only or deprecated_argument("parseOnlyThese", "parse_only")
        if parse_only is not None:
            # Issue a warning if we can tell in advance that
            # parse_only will exclude the entire tree.
            if parse_only.excludes_everything:
                warnings.warn(
                    f"The given value for parse_only will exclude everything: {parse_only}",
                    UserWarning,
                    stacklevel=3,
                )

        from_encoding = from_encoding or deprecated_argument(
            "fromEncoding", "from_encoding"
        )

        if from_encoding and isinstance(markup, str):
            warnings.warn(
                "You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored."
            )
            from_encoding = None

        self.element_classes = element_classes or dict()

        # We need this information to track whether or not the builder
        # was specified well enough that we can omit the 'you need to
        # specify a parser' warning.
        original_builder = builder
        original_features = features

        builder_class: Optional[Type[TreeBuilder]] = None
        if isinstance(builder, type):
            # A builder class was passed in; it needs to be instantiated.
            builder_class = builder
            builder = None
        elif builder is None:
            if isinstance(features, str):
                features = [features]
            if features is None or len(features) == 0:
                features = self.DEFAULT_BUILDER_FEATURES
            possible_builder_class = builder_registry.lookup(*features)
            if possible_builder_class is None:
>               raise FeatureNotFound(
                    "Couldn't find a tree builder with the features you "
                    "requested: %s. Do you need to install a parser library?"
                    % ",".join(features)
                )
E               bs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?

/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/bs4/__init__.py:366: FeatureNotFound
_______________ test_score_candidate_links_prefers_better_match ________________

    def test_score_candidate_links_prefers_better_match():
        html = (
            '<div><a href="/high">My Show Episode</a></div>'
            '<div><a href="/low">Another Show</a></div>'
        )
>       soup = BeautifulSoup(html, "lxml")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/services/test_scraping_service.py:1011:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <[AttributeError("'<class 'bs4.BeautifulSoup'>' object has no attribute 'contents'") raised in repr()] BeautifulSoup object at 0x7ff986400a40>
markup = '<div><a href="/high">My Show Episode</a></div><div><a href="/low">Another Show</a></div>'
features = ['lxml'], builder = None, parse_only = None, from_encoding = None
exclude_encodings = None, element_classes = None, kwargs = {}
deprecated_argument = <function BeautifulSoup.__init__.<locals>.deprecated_argument at 0x7ff9862b0860>
original_builder = None, original_features = 'lxml', builder_class = None
possible_builder_class = None

    def __init__(
        self,
        markup: _IncomingMarkup = "",
        features: Optional[Union[str, Sequence[str]]] = None,
        builder: Optional[Union[TreeBuilder, Type[TreeBuilder]]] = None,
        parse_only: Optional[SoupStrainer] = None,
        from_encoding: Optional[_Encoding] = None,
        exclude_encodings: Optional[_Encodings] = None,
        element_classes: Optional[Dict[Type[PageElement], Type[PageElement]]] = None,
        **kwargs: Any,
    ):
        """Constructor.

        :param markup: A string or a file-like object representing
         markup to be parsed.

        :param features: Desirable features of the parser to be
         used. This may be the name of a specific parser ("lxml",
         "lxml-xml", "html.parser", or "html5lib") or it may be the
         type of markup to be used ("html", "html5", "xml"). It's
         recommended that you name a specific parser, so that
         Beautiful Soup gives you the same results across platforms
         and virtual environments.

        :param builder: A TreeBuilder subclass to instantiate (or
         instance to use) instead of looking one up based on
         `features`. You only need to use this if you've implemented a
         custom TreeBuilder.

        :param parse_only: A SoupStrainer. Only parts of the document
         matching the SoupStrainer will be considered. This is useful
         when parsing part of a document that would otherwise be too
         large to fit into memory.

        :param from_encoding: A string indicating the encoding of the
         document to be parsed. Pass this in if Beautiful Soup is
         guessing wrongly about the document's encoding.

        :param exclude_encodings: A list of strings indicating
         encodings known to be wrong. Pass this in if you don't know
         the document's encoding but you know Beautiful Soup's guess is
         wrong.

        :param element_classes: A dictionary mapping BeautifulSoup
         classes like Tag and NavigableString, to other classes you'd
         like to be instantiated instead as the parse tree is
         built. This is useful for subclassing Tag or NavigableString
         to modify default behavior.

        :param kwargs: For backwards compatibility purposes, the
         constructor accepts certain keyword arguments used in
         Beautiful Soup 3. None of these arguments do anything in
         Beautiful Soup 4; they will result in a warning and then be
         ignored.

         Apart from this, any keyword arguments passed into the
         BeautifulSoup constructor are propagated to the TreeBuilder
         constructor. This makes it possible to configure a
         TreeBuilder by passing in arguments, not just by saying which
         one to use.
        """
        if "convertEntities" in kwargs:
            del kwargs["convertEntities"]
            warnings.warn(
                "BS4 does not respect the convertEntities argument to the "
                "BeautifulSoup constructor. Entities are always converted "
                "to Unicode characters."
            )

        if "markupMassage" in kwargs:
            del kwargs["markupMassage"]
            warnings.warn(
                "BS4 does not respect the markupMassage argument to the "
                "BeautifulSoup constructor. The tree builder is responsible "
                "for any necessary markup massage."
            )

        if "smartQuotesTo" in kwargs:
            del kwargs["smartQuotesTo"]
            warnings.warn(
                "BS4 does not respect the smartQuotesTo argument to the "
                "BeautifulSoup constructor. Smart quotes are always converted "
                "to Unicode characters."
            )

        if "selfClosingTags" in kwargs:
            del kwargs["selfClosingTags"]
            warnings.warn(
                "Beautiful Soup 4 does not respect the selfClosingTags argument to the "
                "BeautifulSoup constructor. The tree builder is responsible "
                "for understanding self-closing tags."
            )

        if "isHTML" in kwargs:
            del kwargs["isHTML"]
            warnings.warn(
                "Beautiful Soup 4 does not respect the isHTML argument to the "
                "BeautifulSoup constructor. Suggest you use "
                "features='lxml' for HTML and features='lxml-xml' for "
                "XML."
            )

        def deprecated_argument(old_name: str, new_name: str) -> Optional[Any]:
            if old_name in kwargs:
                warnings.warn(
                    'The "%s" argument to the BeautifulSoup constructor '
                    'was renamed to "%s" in Beautiful Soup 4.0.0'
                    % (old_name, new_name),
                    DeprecationWarning,
                    stacklevel=3,
                )
                return kwargs.pop(old_name)
            return None

        parse_only = parse_only or deprecated_argument("parseOnlyThese", "parse_only")
        if parse_only is not None:
            # Issue a warning if we can tell in advance that
            # parse_only will exclude the entire tree.
            if parse_only.excludes_everything:
                warnings.warn(
                    f"The given value for parse_only will exclude everything: {parse_only}",
                    UserWarning,
                    stacklevel=3,
                )

        from_encoding = from_encoding or deprecated_argument(
            "fromEncoding", "from_encoding"
        )

        if from_encoding and isinstance(markup, str):
            warnings.warn(
                "You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored."
            )
            from_encoding = None

        self.element_classes = element_classes or dict()

        # We need this information to track whether or not the builder
        # was specified well enough that we can omit the 'you need to
        # specify a parser' warning.
        original_builder = builder
        original_features = features

        builder_class: Optional[Type[TreeBuilder]] = None
        if isinstance(builder, type):
            # A builder class was passed in; it needs to be instantiated.
            builder_class = builder
            builder = None
        elif builder is None:
            if isinstance(features, str):
                features = [features]
            if features is None or len(features) == 0:
                features = self.DEFAULT_BUILDER_FEATURES
            possible_builder_class = builder_registry.lookup(*features)
            if possible_builder_class is None:
>               raise FeatureNotFound(
                    "Couldn't find a tree builder with the features you "
                    "requested: %s. Do you need to install a parser library?"
                    % ",".join(features)
                )
E               bs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?

/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/bs4/__init__.py:366: FeatureNotFound
_______________ test_orchestrate_searches_calls_sites_and_sorts ________________

mocker = <pytest_mock.plugin.MockerFixture object at 0x7ff986413140>

    @pytest.mark.asyncio
    async def test_orchestrate_searches_calls_sites_and_sorts(mocker):
        ctx = _ctx_with_config(
            websites_movies=[
                {
                    "name": "YTS.mx",
                    "enabled": True,
                    "search_url": "https://yts.mx/browse-movies/{query}/all/all/0/latest/0/all",
                },
                {
                    "name": "1337x",
                    "enabled": True,
                    "search_url": "https://1337x.to/category-search/{query}/Movies/1/",
                },
            ]
        )

        yts_results = [
            {"title": "Alien (1979) 1080p", "score": 20, "source": "YTS.mx"},
            {"title": "Alien (1979) 720p", "score": 10, "source": "YTS.mx"},
        ]
        txx_results = [
            {"title": "Alien.1979.1080p", "score": 15, "source": "1337x"},
        ]

>       m_yts = mocker.patch(
            "telegram_bot.services.scraping_service.scrape_yts",
            new=AsyncMock(return_value=yts_results),
        )

tests/services/test_search_orchestration.py:65:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/pytest_mock/plugin.py:448: in __call__
    return self._start_patch(
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/pytest_mock/plugin.py:266: in _start_patch
    mocked: MockType = p.start()
                       ^^^^^^^^^
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1624: in start
    result = self.__enter__()
             ^^^^^^^^^^^^^^^^
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x7ff9864109e0>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'telegram_bot.services.scraping_service' from '/app/telegram_bot/services/scraping_service.py'> does not have the attribute 'scrape_yts'

/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1437: AttributeError
_______________ test_orchestrate_searches_respects_enabled_flag ________________

mocker = <pytest_mock.plugin.MockerFixture object at 0x7ff986400320>

    @pytest.mark.asyncio
    async def test_orchestrate_searches_respects_enabled_flag(mocker):
        ctx = _ctx_with_config(
            websites_movies=[
                {
                    "name": "YTS.mx",
                    "enabled": False,
                    "search_url": "https://yts.mx/browse-movies/{query}",
                },
                {
                    "name": "1337x",
                    "enabled": True,
                    "search_url": "https://1337x.to/category-search/{query}/Movies/1/",
                },
            ]
        )

>       m_yts = mocker.patch(
            "telegram_bot.services.scraping_service.scrape_yts",
            new=AsyncMock(return_value=[]),
        )

tests/services/test_search_orchestration.py:110:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/pytest_mock/plugin.py:448: in __call__
    return self._start_patch(
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/pytest_mock/plugin.py:266: in _start_patch
    mocked: MockType = p.start()
                       ^^^^^^^^^
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1624: in start
    result = self.__enter__()
             ^^^^^^^^^^^^^^^^
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <unittest.mock._patch object at 0x7ff9864122d0>

    def get_original(self):
        target = self.getter()
        name = self.attribute

        original = DEFAULT
        local = False

        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True

        if name in _builtins and isinstance(target, ModuleType):
            self.create = True

        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'telegram_bot.services.scraping_service' from '/app/telegram_bot/services/scraping_service.py'> does not have the attribute 'scrape_yts'

/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1437: AttributeError
___________ test_orchestrate_searches_yaml_fallback_for_unknown_site ___________

mocker = <pytest_mock.plugin.MockerFixture object at 0x7ff9863f6090>

    @pytest.mark.asyncio
    async def test_orchestrate_searches_yaml_fallback_for_unknown_site(mocker):
        ctx = _ctx_with_config(
            websites_movies=[
                {
                    "name": "EZTV",
                    "enabled": True,
                    "search_url": "https://eztv.re/search/{query}",
                },
            ]
        )

        m_yaml = mocker.patch(
            "telegram_bot.services.scraping_service.scrape_yaml_site",
            new=AsyncMock(
                return_value=[{"title": "Alien (1979) EZ", "score": 9, "source": "EZTV"}]
            ),
        )

        results = await orchestrate_searches("Alien", "movie", ctx, year="1979")
>       assert results and results[0]["source"] == "EZTV"
E       assert ([])

tests/services/test_search_orchestration.py:148: AssertionError
------------------------------ Captured log call -------------------------------
WARNING  telegram_bot.config:generic_web_scraper.py:267 [SCRAPER] No YAML config found for site 'EZTV'  skipping.
____________________ test_handle_tv_scope_selection_season _____________________

mocker = <pytest_mock.plugin.MockerFixture object at 0x7ff984dd8110>
context = namespace(bot=namespace(send_message=<AsyncMock id='140709675353200'>, delete_message=<AsyncMock id='140709652622000'>), user_data={}, bot_data={})
make_callback_query = <function make_callback_query.<locals>._make at 0x7ff9862ab4c0>
make_message = <function make_message.<locals>._make at 0x7ff9862abe20>

    @pytest.mark.asyncio
    async def test_handle_tv_scope_selection_season(
        mocker, context, make_callback_query, make_message
    ):
        mocker.patch(
            "telegram_bot.workflows.search_workflow.safe_edit_message", new=AsyncMock()
        )
>       mocker.patch(
            "telegram_bot.workflows.search_workflow.scraping_service.fetch_season_episode_count_from_wikipedia",
            new=AsyncMock(return_value=2),
        )

tests/workflows/test_search_workflow.py:253:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/pytest_mock/plugin.py:448: in __call__
    return self._start_patch(
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/pytest_mock/plugin.py:266: in _start_patch
    mocked: MockType = p.start()
                       ^^^^^^^^^
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1624: in start
    result = self.__enter__()
             ^^^^^^^^^^^^^^^^
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1451: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

name = 'telegram_bot.workflows.search_workflow.scraping_service'

    def resolve_name(name):
        """
        Resolve a name to an object.

        It is expected that `name` will be a string in one of the following
        formats, where W is shorthand for a valid Python identifier and dot stands
        for a literal period in these pseudo-regexes:

        W(.W)*
        W(.W)*:(W(.W)*)?

        The first form is intended for backward compatibility only. It assumes that
        some part of the dotted name is a package, and the rest is an object
        somewhere within that package, possibly nested inside other objects.
        Because the place where the package stops and the object hierarchy starts
        can't be inferred by inspection, repeated attempts to import must be done
        with this form.

        In the second form, the caller makes the division point clear through the
        provision of a single colon: the dotted name to the left of the colon is a
        package to be imported, and the dotted name to the right is the object
        hierarchy within that package. Only one import is needed in this form. If
        it ends with the colon, then a module object is returned.

        The function will return an object (which might be a module), or raise one
        of the following exceptions:

        ValueError - if `name` isn't in a recognised format
        ImportError - if an import failed when it shouldn't have
        AttributeError - if a failure occurred when traversing the object hierarchy
                         within the imported package to get to the desired object.
        """
        global _NAME_PATTERN
        if _NAME_PATTERN is None:
            # Lazy import to speedup Python startup time
            import re
            dotted_words = r'(?!\d)(\w+)(\.(?!\d)(\w+))*'
            _NAME_PATTERN = re.compile(f'^(?P<pkg>{dotted_words})'
                                       f'(?P<cln>:(?P<obj>{dotted_words})?)?$',
                                       re.UNICODE)

        m = _NAME_PATTERN.match(name)
        if not m:
            raise ValueError(f'invalid format: {name!r}')
        gd = m.groupdict()
        if gd.get('cln'):
            # there is a colon - a one-step import is all that's needed
            mod = importlib.import_module(gd['pkg'])
            parts = gd.get('obj')
            parts = parts.split('.') if parts else []
        else:
            # no colon - have to iterate to find the package boundary
            parts = name.split('.')
            modname = parts.pop(0)
            # first part *must* be a module/package.
            mod = importlib.import_module(modname)
            while parts:
                p = parts[0]
                s = f'{modname}.{p}'
                try:
                    mod = importlib.import_module(s)
                    parts.pop(0)
                    modname = s
                except ImportError:
                    break
        # if we reach this point, mod is the module, already imported, and
        # parts is the list of parts in the object hierarchy to be traversed, or
        # an empty list if just the module is wanted.
        result = mod
        for p in parts:
>           result = getattr(result, p)
                     ^^^^^^^^^^^^^^^^^^
E           AttributeError: module 'telegram_bot.workflows.search_workflow' has no attribute 'scraping_service'

/home/jules/.pyenv/versions/3.12.12/lib/python3.12/pkgutil.py:528: AttributeError
________________ test_handle_tv_scope_selection_season_fallback ________________

mocker = <pytest_mock.plugin.MockerFixture object at 0x7ff984d840b0>
context = namespace(bot=namespace(send_message=<AsyncMock id='140709652345344'>, delete_message=<AsyncMock id='140709652688592'>), user_data={}, bot_data={})
make_callback_query = <function make_callback_query.<locals>._make at 0x7ff984d79080>
make_message = <function make_message.<locals>._make at 0x7ff984d79440>

    @pytest.mark.asyncio
    async def test_handle_tv_scope_selection_season_fallback(
        mocker, context, make_callback_query, make_message
    ):
        mocker.patch(
            "telegram_bot.workflows.search_workflow.safe_edit_message", new=AsyncMock()
        )
>       mocker.patch(
            "telegram_bot.workflows.search_workflow.scraping_service.fetch_season_episode_count_from_wikipedia",
            new=AsyncMock(return_value=2),
        )

tests/workflows/test_search_workflow.py:302:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/pytest_mock/plugin.py:448: in __call__
    return self._start_patch(
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/pytest_mock/plugin.py:266: in _start_patch
    mocked: MockType = p.start()
                       ^^^^^^^^^
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1624: in start
    result = self.__enter__()
             ^^^^^^^^^^^^^^^^
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1451: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

name = 'telegram_bot.workflows.search_workflow.scraping_service'

    def resolve_name(name):
        """
        Resolve a name to an object.

        It is expected that `name` will be a string in one of the following
        formats, where W is shorthand for a valid Python identifier and dot stands
        for a literal period in these pseudo-regexes:

        W(.W)*
        W(.W)*:(W(.W)*)?

        The first form is intended for backward compatibility only. It assumes that
        some part of the dotted name is a package, and the rest is an object
        somewhere within that package, possibly nested inside other objects.
        Because the place where the package stops and the object hierarchy starts
        can't be inferred by inspection, repeated attempts to import must be done
        with this form.

        In the second form, the caller makes the division point clear through the
        provision of a single colon: the dotted name to the left of the colon is a
        package to be imported, and the dotted name to the right is the object
        hierarchy within that package. Only one import is needed in this form. If
        it ends with the colon, then a module object is returned.

        The function will return an object (which might be a module), or raise one
        of the following exceptions:

        ValueError - if `name` isn't in a recognised format
        ImportError - if an import failed when it shouldn't have
        AttributeError - if a failure occurred when traversing the object hierarchy
                         within the imported package to get to the desired object.
        """
        global _NAME_PATTERN
        if _NAME_PATTERN is None:
            # Lazy import to speedup Python startup time
            import re
            dotted_words = r'(?!\d)(\w+)(\.(?!\d)(\w+))*'
            _NAME_PATTERN = re.compile(f'^(?P<pkg>{dotted_words})'
                                       f'(?P<cln>:(?P<obj>{dotted_words})?)?$',
                                       re.UNICODE)

        m = _NAME_PATTERN.match(name)
        if not m:
            raise ValueError(f'invalid format: {name!r}')
        gd = m.groupdict()
        if gd.get('cln'):
            # there is a colon - a one-step import is all that's needed
            mod = importlib.import_module(gd['pkg'])
            parts = gd.get('obj')
            parts = parts.split('.') if parts else []
        else:
            # no colon - have to iterate to find the package boundary
            parts = name.split('.')
            modname = parts.pop(0)
            # first part *must* be a module/package.
            mod = importlib.import_module(modname)
            while parts:
                p = parts[0]
                s = f'{modname}.{p}'
                try:
                    mod = importlib.import_module(s)
                    parts.pop(0)
                    modname = s
                except ImportError:
                    break
        # if we reach this point, mod is the module, already imported, and
        # parts is the list of parts in the object hierarchy to be traversed, or
        # an empty list if just the module is wanted.
        result = mod
        for p in parts:
>           result = getattr(result, p)
                     ^^^^^^^^^^^^^^^^^^
E           AttributeError: module 'telegram_bot.workflows.search_workflow' has no attribute 'scraping_service'

/home/jules/.pyenv/versions/3.12.12/lib/python3.12/pkgutil.py:528: AttributeError
______________ test_entire_season_skips_pack_and_targets_missing _______________

mocker = <pytest_mock.plugin.MockerFixture object at 0x7ff984f2af60>
context = namespace(bot=namespace(send_message=<AsyncMock id='140709654287024'>, delete_message=<AsyncMock id='140709654633200'>), user_data={}, bot_data={})
make_callback_query = <function make_callback_query.<locals>._make at 0x7ff984f400e0>
make_message = <function make_message.<locals>._make at 0x7ff984f40220>

    @pytest.mark.asyncio
    async def test_entire_season_skips_pack_and_targets_missing(
        mocker, context, make_callback_query, make_message
    ):
        # Mock messaging and data sources
        mocker.patch(
            "telegram_bot.workflows.search_workflow.safe_edit_message", new=AsyncMock()
        )
>       mocker.patch(
            "telegram_bot.workflows.search_workflow.scraping_service.fetch_season_episode_count_from_wikipedia",
            new=AsyncMock(return_value=5),
        )

tests/workflows/test_search_workflow.py:430:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/pytest_mock/plugin.py:448: in __call__
    return self._start_patch(
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/pytest_mock/plugin.py:266: in _start_patch
    mocked: MockType = p.start()
                       ^^^^^^^^^
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1624: in start
    result = self.__enter__()
             ^^^^^^^^^^^^^^^^
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1451: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

name = 'telegram_bot.workflows.search_workflow.scraping_service'

    def resolve_name(name):
        """
        Resolve a name to an object.

        It is expected that `name` will be a string in one of the following
        formats, where W is shorthand for a valid Python identifier and dot stands
        for a literal period in these pseudo-regexes:

        W(.W)*
        W(.W)*:(W(.W)*)?

        The first form is intended for backward compatibility only. It assumes that
        some part of the dotted name is a package, and the rest is an object
        somewhere within that package, possibly nested inside other objects.
        Because the place where the package stops and the object hierarchy starts
        can't be inferred by inspection, repeated attempts to import must be done
        with this form.

        In the second form, the caller makes the division point clear through the
        provision of a single colon: the dotted name to the left of the colon is a
        package to be imported, and the dotted name to the right is the object
        hierarchy within that package. Only one import is needed in this form. If
        it ends with the colon, then a module object is returned.

        The function will return an object (which might be a module), or raise one
        of the following exceptions:

        ValueError - if `name` isn't in a recognised format
        ImportError - if an import failed when it shouldn't have
        AttributeError - if a failure occurred when traversing the object hierarchy
                         within the imported package to get to the desired object.
        """
        global _NAME_PATTERN
        if _NAME_PATTERN is None:
            # Lazy import to speedup Python startup time
            import re
            dotted_words = r'(?!\d)(\w+)(\.(?!\d)(\w+))*'
            _NAME_PATTERN = re.compile(f'^(?P<pkg>{dotted_words})'
                                       f'(?P<cln>:(?P<obj>{dotted_words})?)?$',
                                       re.UNICODE)

        m = _NAME_PATTERN.match(name)
        if not m:
            raise ValueError(f'invalid format: {name!r}')
        gd = m.groupdict()
        if gd.get('cln'):
            # there is a colon - a one-step import is all that's needed
            mod = importlib.import_module(gd['pkg'])
            parts = gd.get('obj')
            parts = parts.split('.') if parts else []
        else:
            # no colon - have to iterate to find the package boundary
            parts = name.split('.')
            modname = parts.pop(0)
            # first part *must* be a module/package.
            mod = importlib.import_module(modname)
            while parts:
                p = parts[0]
                s = f'{modname}.{p}'
                try:
                    mod = importlib.import_module(s)
                    parts.pop(0)
                    modname = s
                except ImportError:
                    break
        # if we reach this point, mod is the module, already imported, and
        # parts is the list of parts in the object hierarchy to be traversed, or
        # an empty list if just the module is wanted.
        result = mod
        for p in parts:
>           result = getattr(result, p)
                     ^^^^^^^^^^^^^^^^^^
E           AttributeError: module 'telegram_bot.workflows.search_workflow' has no attribute 'scraping_service'

/home/jules/.pyenv/versions/3.12.12/lib/python3.12/pkgutil.py:528: AttributeError
___________________ test_entire_season_all_owned_exits_early ___________________

mocker = <pytest_mock.plugin.MockerFixture object at 0x7ff9863adb50>
context = namespace(bot=namespace(send_message=<AsyncMock id='140709675596832'>, delete_message=<AsyncMock id='140709675455392'>), user_data={}, bot_data={})
make_callback_query = <function make_callback_query.<locals>._make at 0x7ff9862ab380>
make_message = <function make_message.<locals>._make at 0x7ff9862ab740>

    @pytest.mark.asyncio
    async def test_entire_season_all_owned_exits_early(
        mocker, context, make_callback_query, make_message
    ):
        edit_mock = mocker.patch(
            "telegram_bot.workflows.search_workflow.safe_edit_message", new=AsyncMock()
        )
>       mocker.patch(
            "telegram_bot.workflows.search_workflow.scraping_service.fetch_season_episode_count_from_wikipedia",
            new=AsyncMock(return_value=3),
        )

tests/workflows/test_search_workflow.py:510:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/pytest_mock/plugin.py:448: in __call__
    return self._start_patch(
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/pytest_mock/plugin.py:266: in _start_patch
    mocked: MockType = p.start()
                       ^^^^^^^^^
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1624: in start
    result = self.__enter__()
             ^^^^^^^^^^^^^^^^
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1451: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

name = 'telegram_bot.workflows.search_workflow.scraping_service'

    def resolve_name(name):
        """
        Resolve a name to an object.

        It is expected that `name` will be a string in one of the following
        formats, where W is shorthand for a valid Python identifier and dot stands
        for a literal period in these pseudo-regexes:

        W(.W)*
        W(.W)*:(W(.W)*)?

        The first form is intended for backward compatibility only. It assumes that
        some part of the dotted name is a package, and the rest is an object
        somewhere within that package, possibly nested inside other objects.
        Because the place where the package stops and the object hierarchy starts
        can't be inferred by inspection, repeated attempts to import must be done
        with this form.

        In the second form, the caller makes the division point clear through the
        provision of a single colon: the dotted name to the left of the colon is a
        package to be imported, and the dotted name to the right is the object
        hierarchy within that package. Only one import is needed in this form. If
        it ends with the colon, then a module object is returned.

        The function will return an object (which might be a module), or raise one
        of the following exceptions:

        ValueError - if `name` isn't in a recognised format
        ImportError - if an import failed when it shouldn't have
        AttributeError - if a failure occurred when traversing the object hierarchy
                         within the imported package to get to the desired object.
        """
        global _NAME_PATTERN
        if _NAME_PATTERN is None:
            # Lazy import to speedup Python startup time
            import re
            dotted_words = r'(?!\d)(\w+)(\.(?!\d)(\w+))*'
            _NAME_PATTERN = re.compile(f'^(?P<pkg>{dotted_words})'
                                       f'(?P<cln>:(?P<obj>{dotted_words})?)?$',
                                       re.UNICODE)

        m = _NAME_PATTERN.match(name)
        if not m:
            raise ValueError(f'invalid format: {name!r}')
        gd = m.groupdict()
        if gd.get('cln'):
            # there is a colon - a one-step import is all that's needed
            mod = importlib.import_module(gd['pkg'])
            parts = gd.get('obj')
            parts = parts.split('.') if parts else []
        else:
            # no colon - have to iterate to find the package boundary
            parts = name.split('.')
            modname = parts.pop(0)
            # first part *must* be a module/package.
            mod = importlib.import_module(modname)
            while parts:
                p = parts[0]
                s = f'{modname}.{p}'
                try:
                    mod = importlib.import_module(s)
                    parts.pop(0)
                    modname = s
                except ImportError:
                    break
        # if we reach this point, mod is the module, already imported, and
        # parts is the list of parts in the object hierarchy to be traversed, or
        # an empty list if just the module is wanted.
        result = mod
        for p in parts:
>           result = getattr(result, p)
                     ^^^^^^^^^^^^^^^^^^
E           AttributeError: module 'telegram_bot.workflows.search_workflow' has no attribute 'scraping_service'

/home/jules/.pyenv/versions/3.12.12/lib/python3.12/pkgutil.py:528: AttributeError
_________ test_tv_season_fallback_uses_wiki_titles_and_corrected_title _________

mocker = <pytest_mock.plugin.MockerFixture object at 0x7ff984fb2720>

    @pytest.mark.asyncio
    async def test_tv_season_fallback_uses_wiki_titles_and_corrected_title(mocker):
        # Mock episode titles and corrected show title from Wikipedia
>       mocker.patch(
            "telegram_bot.workflows.search_workflow.scraping_service.fetch_episode_titles_for_season",
            new=AsyncMock(return_value=({1: "Pilot"}, "Show (TV series)")),
        )

tests/workflows/test_search_workflow_integration.py:14:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/pytest_mock/plugin.py:448: in __call__
    return self._start_patch(
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/site-packages/pytest_mock/plugin.py:266: in _start_patch
    mocked: MockType = p.start()
                       ^^^^^^^^^
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1624: in start
    result = self.__enter__()
             ^^^^^^^^^^^^^^^^
/home/jules/.pyenv/versions/3.12.12/lib/python3.12/unittest/mock.py:1451: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

name = 'telegram_bot.workflows.search_workflow.scraping_service'

    def resolve_name(name):
        """
        Resolve a name to an object.

        It is expected that `name` will be a string in one of the following
        formats, where W is shorthand for a valid Python identifier and dot stands
        for a literal period in these pseudo-regexes:

        W(.W)*
        W(.W)*:(W(.W)*)?

        The first form is intended for backward compatibility only. It assumes that
        some part of the dotted name is a package, and the rest is an object
        somewhere within that package, possibly nested inside other objects.
        Because the place where the package stops and the object hierarchy starts
        can't be inferred by inspection, repeated attempts to import must be done
        with this form.

        In the second form, the caller makes the division point clear through the
        provision of a single colon: the dotted name to the left of the colon is a
        package to be imported, and the dotted name to the right is the object
        hierarchy within that package. Only one import is needed in this form. If
        it ends with the colon, then a module object is returned.

        The function will return an object (which might be a module), or raise one
        of the following exceptions:

        ValueError - if `name` isn't in a recognised format
        ImportError - if an import failed when it shouldn't have
        AttributeError - if a failure occurred when traversing the object hierarchy
                         within the imported package to get to the desired object.
        """
        global _NAME_PATTERN
        if _NAME_PATTERN is None:
            # Lazy import to speedup Python startup time
            import re
            dotted_words = r'(?!\d)(\w+)(\.(?!\d)(\w+))*'
            _NAME_PATTERN = re.compile(f'^(?P<pkg>{dotted_words})'
                                       f'(?P<cln>:(?P<obj>{dotted_words})?)?$',
                                       re.UNICODE)

        m = _NAME_PATTERN.match(name)
        if not m:
            raise ValueError(f'invalid format: {name!r}')
        gd = m.groupdict()
        if gd.get('cln'):
            # there is a colon - a one-step import is all that's needed
            mod = importlib.import_module(gd['pkg'])
            parts = gd.get('obj')
            parts = parts.split('.') if parts else []
        else:
            # no colon - have to iterate to find the package boundary
            parts = name.split('.')
            modname = parts.pop(0)
            # first part *must* be a module/package.
            mod = importlib.import_module(modname)
            while parts:
                p = parts[0]
                s = f'{modname}.{p}'
                try:
                    mod = importlib.import_module(s)
                    parts.pop(0)
                    modname = s
                except ImportError:
                    break
        # if we reach this point, mod is the module, already imported, and
        # parts is the list of parts in the object hierarchy to be traversed, or
        # an empty list if just the module is wanted.
        result = mod
        for p in parts:
>           result = getattr(result, p)
                     ^^^^^^^^^^^^^^^^^^
E           AttributeError: module 'telegram_bot.workflows.search_workflow' has no attribute 'scraping_service'

/home/jules/.pyenv/versions/3.12.12/lib/python3.12/pkgutil.py:528: AttributeError
=========================== short test summary info ============================
FAILED tests/services/test_dry_run_integration.py::test_dry_run_flow_uses_wiki_year_and_filters_resolution
FAILED tests/services/test_dry_run_integration.py::test_dry_run_movie_explicit_year_overrides_wiki
FAILED tests/services/test_dry_run_integration.py::test_dry_run_tv_search_workflow_basic
FAILED tests/services/test_generic_torrent_scraper_filtering.py::test_two_stage_filtering_prefers_precise_single_token_match
FAILED tests/services/test_generic_torrent_scraper_magnet.py::test_search_parses_magnet_link_from_detail_page
FAILED tests/services/test_generic_torrent_scraper_topn.py::test_extract_data_from_row
FAILED tests/services/test_generic_torrent_scraper_topn.py::test_parse_and_select_top_results
FAILED tests/services/test_scraping_service.py::test_fetch_episode_title_dedicated_page
FAILED tests/services/test_scraping_service.py::test_fetch_episode_title_strips_miniseries_suffix
FAILED tests/services/test_scraping_service.py::test_fetch_episode_title_strips_tv_series_suffix
FAILED tests/services/test_scraping_service.py::test_fetch_episode_title_embedded_page
FAILED tests/services/test_scraping_service.py::test_fetch_episode_title_not_found
FAILED tests/services/test_scraping_service.py::test_fetch_season_episode_count
FAILED tests/services/test_scraping_service.py::test_fetch_season_episode_count_prefers_titles_over_overview
FAILED tests/services/test_scraping_service.py::test_fetch_season_episode_count_skips_ongoing_overview
FAILED tests/services/test_scraping_service.py::test_scrape_1337x_parses_results
FAILED tests/services/test_scraping_service.py::test_scrape_1337x_no_results
FAILED tests/services/test_scraping_service.py::test_scrape_1337x_fuzzy_filter
FAILED tests/services/test_scraping_service.py::test_scrape_1337x_passes_limit
FAILED tests/services/test_scraping_service.py::test_scrape_yts_parses_results
FAILED tests/services/test_scraping_service.py::test_scrape_yts_retries_on_validation_failure
FAILED tests/services/test_scraping_service.py::test_scrape_yts_paginates_browse_pages_to_find_year
FAILED tests/services/test_scraping_service.py::test_scrape_yts_api_fallback_relaxes_quality
FAILED tests/services/test_scraping_service.py::test_scrape_yts_api_fallback_relaxes_year
FAILED tests/services/test_scraping_service.py::test_scrape_yts_token_gate_avoids_near_homonyms
FAILED tests/services/test_scraping_service.py::test_strategy_find_direct_links_magnet
FAILED tests/services/test_scraping_service.py::test_strategy_find_direct_links_torrent
FAILED tests/services/test_scraping_service.py::test_strategy_find_direct_links_none
FAILED tests/services/test_scraping_service.py::test_strategy_contextual_search_keyword
FAILED tests/services/test_scraping_service.py::test_strategy_contextual_search_query_match
FAILED tests/services/test_scraping_service.py::test_strategy_contextual_search_unrelated_keyword
FAILED tests/services/test_scraping_service.py::test_strategy_find_in_tables_single_match
FAILED tests/services/test_scraping_service.py::test_strategy_find_in_tables_multiple_matches
FAILED tests/services/test_scraping_service.py::test_strategy_find_in_tables_ignores_unrelated_tables
FAILED tests/services/test_scraping_service.py::test_score_candidate_links_prefers_magnet
FAILED tests/services/test_scraping_service.py::test_score_candidate_links_penalizes_ads
FAILED tests/services/test_scraping_service.py::test_score_candidate_links_prefers_better_match
FAILED tests/services/test_search_orchestration.py::test_orchestrate_searches_calls_sites_and_sorts
FAILED tests/services/test_search_orchestration.py::test_orchestrate_searches_respects_enabled_flag
FAILED tests/services/test_search_orchestration.py::test_orchestrate_searches_yaml_fallback_for_unknown_site
FAILED tests/workflows/test_search_workflow.py::test_handle_tv_scope_selection_season
FAILED tests/workflows/test_search_workflow.py::test_handle_tv_scope_selection_season_fallback
FAILED tests/workflows/test_search_workflow.py::test_entire_season_skips_pack_and_targets_missing
FAILED tests/workflows/test_search_workflow.py::test_entire_season_all_owned_exits_early
FAILED tests/workflows/test_search_workflow_integration.py::test_tv_season_fallback_uses_wiki_titles_and_corrected_title
======================== 45 failed, 112 passed in 8.93s ========================
